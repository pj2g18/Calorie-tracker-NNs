{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467c7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import random\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf722e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['78.1499', '76.7501', '74.85', '75.95', '76.8', '76.1001', '75.3', '75.5998', '75.5', '74.2', '75.2002', '76.1999', '76.1001', '75.2501', '75.0001', '74.6999', '76.1001', '75.4501', '75.5998', '76', '76.35', '74.65', '74.5', '74', '74', '73.7001', '72.1502', '73.4502', '72.9998', '72.45', '72.1502', '73.4502', '73.8498', '73.2', '72.45', '72.5498']\n"
     ]
    }
   ],
   "source": [
    "startDate = '03-11-2021' \n",
    "endDate = '04-16-2021'\n",
    "\n",
    "#get time period\n",
    "startd = datetime.datetime.strptime(startDate, \"%m-%d-%Y\")\n",
    "endd = datetime.datetime.strptime(endDate, \"%m-%d-%Y\")\n",
    "\n",
    "#gets range of dates between start and end\n",
    "date_array = (startd + datetime.timedelta(days=x) for x in range(0, (endd-startd).days))\n",
    "dates = []\n",
    "for date_object in date_array:\n",
    "    #gets date value as string\n",
    "    dates = np.append(dates, date_object.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "lastDate = \"hi\"\n",
    "\n",
    "weights = []\n",
    "with open('data/BodyMass.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "    # for every row, print the row\n",
    "    for row in csvReader:\n",
    "        if row[7][:10] in dates and row[7][:10] != lastDate:\n",
    "            lastDate = row[7][:10]\n",
    "            weights.append(row[8])\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a418c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.09', '0.093', '0.09', '0.109', '0.096', '0.078', '0.084', '0.075', '0.081', '0.084', '0.084', '0.09', '0.087', '0.087', '0.09', '0.102', '0.102', '0.087', '0.087', '0.084', '0.081', '0.087', '0.087', '0.087', '0.087', '0.084', '0.084', '0.084', '0.081', '0.084', '0.084', '0.084', '0.078', '0.078', '0.078', '0.084']\n"
     ]
    }
   ],
   "source": [
    "with open('data/BodyFatPercentage.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    fats = []\n",
    "    # for every row, print the row\n",
    "    for row in csvReader:\n",
    "        if row[7][:10] in dates:\n",
    "            fats.append(row[8])\n",
    "        \n",
    "\n",
    "print(fats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19024753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2172.2921, 2740.02, 2966.107, 2488.631, 2304.759, 2708.63, 2722.84, 1952.1239999999998, 3078.19, 2231.268, 3353.6, 3142.65, 2235.87, 2270.07, 3440.352, 2502.3900000000003, 3255.68, 2590.7200000000003, 3854.34, 2444.58, 3160.02, 2817.0, 3037.5, 2931.607, 1918.3, 2441.45, 1179.0, 3116.8, 1675.38, 1946.93, 2552.65, 1852.56, 3265.92, 1431.9, 2216.3, 1698.6929999999998]\n",
      "[ -97.59771528 -134.50124951  294.2354425   223.64329007  -54.34421525\n",
      "  -54.55694239   78.28471411   -5.32694669  -90.56318653  265.2020374\n",
      "  265.88048891   -7.86961556  -59.7753824   -16.74311621  -17.77651284\n",
      "  371.30634911  -49.96142398   39.6936031   105.71031339   92.39584086\n",
      " -117.84484622  -10.5485969   -35.16198965   -0.          -21.9135168\n",
      " -108.9778205   344.69370988  -32.48442041  -37.84244975  -21.07977972\n",
      "  344.69370988  105.16753048  -45.67474364  -52.71784815   27.23298529\n",
      "    0.        ]\n",
      "[2269.8898152776296, 2874.5212495072706, 2671.871557504999, 2264.987709925002, 2359.10321525358, 2763.1869423864205, 2644.5552858884976, 1957.4509466915, 3168.7531865299993, 1966.065962598485, 3087.719511086617, 3150.51961555697, 2295.645382405, 2286.81311620783, 3458.1285128388804, 2131.0836508924426, 3305.6414239808487, 2551.0263969037915, 3748.6296866141397, 2352.1841591350003, 3277.864846219999, 2827.5485968950006, 3072.66198965, 2931.607, 1940.2135167992392, 2550.427820499241, 834.3062901200008, 3149.2844204087, 1713.2224497528189, 1968.0097797184812, 2207.9562901200006, 1747.3924695243568, 3311.59474363716, 1484.61784815, 2189.067014711519, 1698.6929999999998]\n"
     ]
    }
   ],
   "source": [
    "with open('data/DietaryEnergyConsumed.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    calories = []\n",
    "    calorie = 0\n",
    "    lastDate = \"2021-03-11\"\n",
    "    # for every row, print the row\n",
    "    for row in csvReader:\n",
    "        if row[7][:10] in dates:\n",
    "            if row[7][:10] != lastDate:\n",
    "                calories.append(calorie)\n",
    "                calorie = float(row[8])\n",
    "            else:\n",
    "                calorie += float(row[8])\n",
    "            lastDate = row[7][:10]\n",
    "        \n",
    "calories.append(calorie)\n",
    "        \n",
    "\n",
    "print(calories)\n",
    "\n",
    "def caloriesBurned(weight, fat):\n",
    "    fatMass = []\n",
    "    leanMass = []\n",
    "    for x in range(len(weight)):\n",
    "        fatMass = np.append(fatMass, float(weight[x])*float(fat[x])/100)\n",
    "        leanMass = np.append(leanMass, float(weight[x]) - fatMass[x])\n",
    "    calories = [] \n",
    "    for x in range(len(weight) - 1):   \n",
    "        calories = np.append(calories, -(fatMass[x]-fatMass[x+1])*442.39)\n",
    "        if leanMass[x] >= leanMass[x+1]:\n",
    "            calories[x] -= (leanMass[x] - leanMass[x+1])*70\n",
    "        else:\n",
    "            calories[x] -= (leanMass[x]-leanMass[x+1])*265\n",
    "    calories = np.append(calories, [0])\n",
    "\n",
    "    return calories\n",
    "        \n",
    "#calculate total calories burned in a day\n",
    "calorieDiff = caloriesBurned(weights, fats)\n",
    "print(calorieDiff)\n",
    "for x in range(len(calories)):\n",
    "    calories[x] = calories[x] - calorieDiff[x]\n",
    "\n",
    "#calories = np.append(calories, [2000,2000, 2000])\n",
    "print(calories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e575b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "rhr = []\n",
    "rhpos = []\n",
    "with open('data/RestingHeartRate.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    \n",
    "    for row in csvReader:\n",
    "        if row[7][:10] in dates:\n",
    "            dt = row[7][:19]            \n",
    "            dt = datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "            rhr.append(row[8])\n",
    "            rhpos.append(dt)\n",
    "            \n",
    "            \n",
    "print(np.size(dates))\n",
    "print(np.size(rhr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c631259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021-03-11' '2021-03-12' '2021-03-13' '2021-03-14' '2021-03-15'\n",
      " '2021-03-16' '2021-03-17' '2021-03-18' '2021-03-19' '2021-03-20'\n",
      " '2021-03-21' '2021-03-22' '2021-03-23' '2021-03-24' '2021-03-25'\n",
      " '2021-03-26' '2021-03-27' '2021-03-28' '2021-03-29' '2021-03-30'\n",
      " '2021-03-31' '2021-04-01' '2021-04-02' '2021-04-03' '2021-04-04'\n",
      " '2021-04-05' '2021-04-06' '2021-04-07' '2021-04-08' '2021-04-09'\n",
      " '2021-04-10' '2021-04-11' '2021-04-12' '2021-04-13' '2021-04-14'\n",
      " '2021-04-15']\n",
      "127540\n",
      "127540\n",
      "127540\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "#things to fill out\n",
    "rootdir = \"data\"\n",
    "startDate = '03-11-2021' \n",
    "endDate = '04-16-2021'\n",
    "\n",
    "#get time period\n",
    "startd = datetime.datetime.strptime(startDate, \"%m-%d-%Y\")\n",
    "endd = datetime.datetime.strptime(endDate, \"%m-%d-%Y\")\n",
    "\n",
    "#gets range of dates between start and end\n",
    "date_array = (startd + datetime.timedelta(days=x) for x in range(0, (endd-startd).days))\n",
    "dates = []\n",
    "for date_object in date_array:\n",
    "    #gets date value as string\n",
    "    dates = np.append(dates, date_object.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "#get weight data\n",
    "roughdata = []\n",
    "hr = []\n",
    "times = []\n",
    "count = 0\n",
    "lastDate = dates[0]\n",
    "dts = []\n",
    "lastRow = []\n",
    "totaltimes = []\n",
    "day = -1\n",
    "lastDate = \"hi\"\n",
    "with open('data/HeartRate.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "    # for every row, print the row\n",
    "    print(dates)\n",
    "    for row in csvReader:\n",
    "      #  print(row[7][:10])\n",
    "    \n",
    "        if row[7][:10] in dates:       \n",
    "                \n",
    "            dt = row[7][:19]            \n",
    "            dtime = datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            dts.append(dtime)\n",
    "            hr.append(row[8])\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "splitInputs = []\n",
    "lastSplit = 0\n",
    "maxinterval = 30\n",
    "done = False\n",
    "counter = 0\n",
    "day = 0\n",
    "while done == False:\n",
    "    counter += 1\n",
    "    if counter < len(dts):\n",
    "        if (dts[counter] - dts[counter - 1]).total_seconds() > maxinterval:\n",
    "            dts.insert(counter, dts[counter - 1] + timedelta(seconds = maxinterval))\n",
    "            date = dts[counter].strftime('%Y-%m-%d')\n",
    "            \n",
    "            \n",
    "            #insert new indices with resting heart rate\n",
    "            index = np.where(dates == date)\n",
    "            hr.insert(counter, rhr[index[0][0]])\n",
    "    else:\n",
    "        done = True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "totsecs = []\n",
    "splitInputs = []\n",
    "lastSplit = 0\n",
    "for x in range(len(dts)-1):\n",
    "    totsecs.append((dts[x+1] - dts[x]).total_seconds())\n",
    "    if dts[x].strftime(\"%m/%d/%y\") != dts[x+1].strftime(\"%m/%d/%y\"):\n",
    "        splitInputs.append(x- lastSplit)\n",
    "        lastSplit = x\n",
    "\n",
    "\n",
    "splitInputs.append(len(hr) - lastSplit)\n",
    "totsecs.append(5)\n",
    "    \n",
    "hr = np.asarray(hr).astype(float)\n",
    "print(len(hr))\n",
    "print(len(dts))\n",
    "print(np.sum(splitInputs))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0d8fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1.382943 2.765886 4.148829 ... 0.       0.       0.      ]\n"
     ]
    }
   ],
   "source": [
    "startDate = '03-11-2021' \n",
    "endDate = '04-16-2021'\n",
    "import math\n",
    "#get time period\n",
    "startd = datetime.datetime.strptime(startDate, \"%m-%d-%Y\")\n",
    "endd = datetime.datetime.strptime(endDate, \"%m-%d-%Y\")\n",
    "#gets range of dates between start and end\n",
    "date_array = (startd + datetime.timedelta(days=x) for x in range(0, (endd-startd).days))\n",
    "dates = []\n",
    "\n",
    "for date_object in date_array:\n",
    "    #gets date value as string\n",
    "    dates = np.append(dates, date_object.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "roughdata = []\n",
    "steps = []\n",
    "times = []\n",
    "count = 0\n",
    "lastDate = dates[0]\n",
    "stepdts = []\n",
    "lastRow = []\n",
    "totaltimes = []\n",
    "date = []\n",
    "day = -1\n",
    "lastDate = \"hi\"\n",
    "    \n",
    "steps = []    \n",
    "with open('data/DistanceWalkingRunning.csv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "    # for every row, print the row\n",
    "    count = 0\n",
    "    for row in csvReader:\n",
    "        if row[7][:10] in dates: \n",
    "            dt = row[7][:19]            \n",
    "            dtime = datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "            stepdts.append(dtime)\n",
    "            steps.append(row[8])\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "startstep = 0\n",
    "\n",
    "while stepdts[startstep] <= dts[0]:\n",
    "    startstep+=1 \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "hrsteps = np.zeros(len(hr))\n",
    "print(hrsteps)\n",
    "\n",
    "\n",
    "     \n",
    "steppos = startstep\n",
    "hrpos = 0\n",
    "hrdate = dts[hrpos]\n",
    "stepdate = stepdts[steppos]\n",
    "\n",
    "\n",
    "while steppos != len(stepdts) - 1:    \n",
    "    if steppos == 0:\n",
    "        minstep = float(steps[steppos])\n",
    "    else:\n",
    "        \n",
    "        if stepdts[steppos] == stepdts[steppos - 1]:\n",
    "            minstep = float(steps[steppos]) / (stepdts[steppos] - stepdts[steppos - 2]).total_seconds()\n",
    "        else:\n",
    "            minstep = float(steps[steppos]) / (stepdts[steppos] - stepdts[steppos - 1]).total_seconds()\n",
    "\n",
    "\n",
    "    if stepdts[steppos] < dts[hrpos+1]:\n",
    "        hrsteps[hrpos] += minstep * (stepdts[steppos] - hrdate).total_seconds()\n",
    "        hrdate = stepdts[steppos]\n",
    "        steppos += 1\n",
    "    else:\n",
    "        hrsteps[hrpos] += minstep * (dts[hrpos+1] - hrdate).total_seconds()\n",
    "        hrpos += 1\n",
    "        if dts[hrpos + 1] == stepdts[steppos]:\n",
    "            steppos += 1\n",
    "              \n",
    "\n",
    "print(hrsteps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eadc36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1770. 1560. 1560. ... 1470.  441.  335.]\n",
      "[0.85061801 0.74969723 0.74969723 ... 0.55142728 0.16542818 0.1256654 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hrinterval = np.multiply(hr,totsecs)\n",
    "print(hrinterval)\n",
    "'''for x in range(len(splitInputs)):\n",
    "    if x == 0:\n",
    "        number = np.sum(hrinterval[:splitInputs[x]])\n",
    "        hrinterval[:splitInputs[x]] = (hrinterval[:splitInputs[x]] / number) * calories[x].detach().numpy()\n",
    "\n",
    "    else:\n",
    "        number = np.sum(hrinterval[np.sum(splitInputs[x-1]):np.sum(splitInputs[x-1] + splitInputs[x])]) \n",
    "        hrinterval[np.sum(splitInputs[x-1]):np.sum(splitInputs[x-1] + splitInputs[x])] = (hrinterval[np.sum(splitInputs[x-1]):np.sum(splitInputs[x-1] + splitInputs[x])] / number) * calories[x].detach().numpy()\n",
    "'''                     \n",
    "total = 0\n",
    "for y in range(len(splitInputs)):\n",
    "    summed = np.sum(hrinterval[total : total + splitInputs[y]])\n",
    "    if y == 0:\n",
    "        for x in range(splitInputs[y]):\n",
    "            hrinterval[x] = hrinterval[x] / summed * calories[y]\n",
    "    else:\n",
    "        for x in range(splitInputs[y]):\n",
    "            hrinterval[np.sum(splitInputs[:y]) + x] = hrinterval[np.sum(splitInputs[:y]) + x] / summed * calories[y]\n",
    "        \n",
    "        \n",
    "    total += splitInputs[y]   \n",
    "        \n",
    "\n",
    "print(hrinterval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0403018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59. 52. 52. ... 49. 49. 67.]\n",
      "tensor([[-0.8084,  1.0000, -0.9408],\n",
      "        [-0.8922,  1.0000, -0.8815],\n",
      "        [-0.8922,  1.0000, -0.8223],\n",
      "        ...,\n",
      "        [-0.9281,  1.0000, -1.0000],\n",
      "        [-0.9281, -0.4000, -1.0000],\n",
      "        [-0.7126, -0.6667, -1.0000]])\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "print(hr)\n",
    "normalized_hr = torch.FloatTensor(scaler.fit_transform(np.reshape(hr, (-1,1)))) \n",
    "normalized_interval =  torch.FloatTensor(scaler.fit_transform(np.reshape(totsecs, (-1,1))))\n",
    "normalized_steps =  torch.FloatTensor(scaler.fit_transform(np.reshape(hrsteps, (-1,1))))\n",
    "\n",
    "hr = np.hstack((normalized_hr, normalized_interval))\n",
    "hr = np.hstack((hr, normalized_steps))\n",
    "hr = torch.FloatTensor(hr)\n",
    "print(hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3269a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2269.88981528]\n",
      " [2874.52124951]\n",
      " [2671.8715575 ]\n",
      " [2264.98770993]\n",
      " [2359.10321525]\n",
      " [2763.18694239]\n",
      " [2644.55528589]\n",
      " [1957.45094669]\n",
      " [3168.75318653]\n",
      " [1966.0659626 ]\n",
      " [3087.71951109]\n",
      " [3150.51961556]\n",
      " [2295.6453824 ]\n",
      " [2286.81311621]\n",
      " [3458.12851284]\n",
      " [2131.08365089]\n",
      " [3305.64142398]\n",
      " [2551.0263969 ]\n",
      " [3748.62968661]\n",
      " [2352.18415914]\n",
      " [3277.86484622]\n",
      " [2827.5485969 ]\n",
      " [3072.66198965]\n",
      " [2931.607     ]\n",
      " [1940.2135168 ]\n",
      " [2550.4278205 ]\n",
      " [ 834.30629012]\n",
      " [3149.28442041]\n",
      " [1713.22244975]\n",
      " [1968.00977972]\n",
      " [2207.95629012]\n",
      " [1747.39246952]\n",
      " [3311.59474364]\n",
      " [1484.61784815]\n",
      " [2189.06701471]\n",
      " [1698.693     ]]\n",
      "torch.Size([36, 1])\n",
      "tensor([[-0.0148],\n",
      "        [ 0.4001],\n",
      "        [ 0.2611],\n",
      "        [-0.0182],\n",
      "        [ 0.0464],\n",
      "        [ 0.3237],\n",
      "        [ 0.2423],\n",
      "        [-0.2292],\n",
      "        [ 0.6021],\n",
      "        [-0.2233],\n",
      "        [ 0.5464],\n",
      "        [ 0.5895],\n",
      "        [ 0.0029],\n",
      "        [-0.0032],\n",
      "        [ 0.8006],\n",
      "        [-0.1101],\n",
      "        [ 0.6960],\n",
      "        [ 0.1781],\n",
      "        [ 1.0000],\n",
      "        [ 0.0417],\n",
      "        [ 0.6769],\n",
      "        [ 0.3679],\n",
      "        [ 0.5361],\n",
      "        [ 0.4393],\n",
      "        [-0.2411],\n",
      "        [ 0.1777],\n",
      "        [-1.0000],\n",
      "        [ 0.5887],\n",
      "        [-0.3968],\n",
      "        [-0.2220],\n",
      "        [-0.0573],\n",
      "        [-0.3734],\n",
      "        [ 0.7001],\n",
      "        [-0.5537],\n",
      "        [-0.0703],\n",
      "        [-0.4068]])\n",
      "[[2269.88981537]\n",
      " [2874.52123553]\n",
      " [2671.87153723]\n",
      " [2264.98771102]\n",
      " [2359.10321423]\n",
      " [2763.18694262]\n",
      " [2644.55529665]\n",
      " [1957.45095252]\n",
      " [3168.7531535 ]\n",
      " [1966.06596164]\n",
      " [3087.71952134]\n",
      " [3150.5196287 ]\n",
      " [2295.64538249]\n",
      " [2286.8131162 ]\n",
      " [3458.12852184]\n",
      " [2131.08365379]\n",
      " [3305.6413839 ]\n",
      " [2551.02640541]\n",
      " [3748.62968661]\n",
      " [2352.18415746]\n",
      " [3277.86481921]\n",
      " [2827.54859096]\n",
      " [3072.66197233]\n",
      " [2931.60698363]\n",
      " [1940.21350829]\n",
      " [2550.42781036]\n",
      " [ 834.30629012]\n",
      " [3149.28439672]\n",
      " [1713.22243568]\n",
      " [1968.00978875]\n",
      " [2207.95629256]\n",
      " [1747.39246797]\n",
      " [3311.59476429]\n",
      " [1484.61788364]\n",
      " [2189.06701219]\n",
      " [1698.69300002]]\n"
     ]
    }
   ],
   "source": [
    "#calculate expected outputs\n",
    "y_actual = []\n",
    "day = 0\n",
    "\n",
    "\n",
    "print(np.reshape(calories, (-1,1)))\n",
    "\n",
    "outputScaler = MinMaxScaler(feature_range=(-1,1))\n",
    "normalized_y = torch.FloatTensor(outputScaler.fit_transform(np.reshape(calories,(-1,1))))\n",
    "print(np.shape(normalized_y))\n",
    "\n",
    "outputs = normalized_y\n",
    "print(outputs)\n",
    "print(outputScaler.inverse_transform(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f259cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size = 3, hidden_size = 5, output_size = 1):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2240863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (in2hidden): Linear(in_features=8, out_features=5, bias=True)\n",
      "  (in2output): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = MyRNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb875c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e45661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\.conda\\envs\\ailab\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.5)   # optimize all cnn parameters\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b3bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "tensor([[-0.0148],\n",
      "        [ 0.4001],\n",
      "        [ 0.2611],\n",
      "        [-0.0182],\n",
      "        [ 0.0464],\n",
      "        [ 0.3237],\n",
      "        [ 0.2423],\n",
      "        [-0.2292],\n",
      "        [ 0.6021],\n",
      "        [-0.2233],\n",
      "        [ 0.5464],\n",
      "        [ 0.5895],\n",
      "        [ 0.0029],\n",
      "        [-0.0032],\n",
      "        [ 0.8006],\n",
      "        [-0.1101],\n",
      "        [ 0.6960],\n",
      "        [ 0.1781],\n",
      "        [ 1.0000],\n",
      "        [ 0.0417],\n",
      "        [ 0.6769],\n",
      "        [ 0.3679],\n",
      "        [ 0.5361],\n",
      "        [ 0.4393],\n",
      "        [-0.2411],\n",
      "        [ 0.1777],\n",
      "        [-1.0000],\n",
      "        [ 0.5887],\n",
      "        [-0.3968],\n",
      "        [-0.2220],\n",
      "        [-0.0573],\n",
      "        [-0.3734],\n",
      "        [ 0.7001],\n",
      "        [-0.5537],\n",
      "        [-0.0703],\n",
      "        [-0.4068]])\n",
      "[0.4965104825174895]\n",
      "Epoch:  0 | train loss: 0.2814\n",
      "[0.4965104825174895]\n",
      "Epoch:  1 | train loss: 0.4510\n",
      "[0.49601397203497205]\n",
      "Epoch:  2 | train loss: 0.5487\n",
      "[0.49601397203497205]\n",
      "Epoch:  3 | train loss: 0.5933\n",
      "[0.4955179580629371]\n",
      "Epoch:  4 | train loss: 0.6095\n",
      "[0.4955179580629371]\n",
      "Epoch:  5 | train loss: 0.6154\n",
      "[0.4955179580629371]\n",
      "Epoch:  6 | train loss: 0.6178\n",
      "[0.4950224401048741]\n",
      "Epoch:  7 | train loss: 0.6171\n",
      "[0.4950224401048741]\n",
      "Epoch:  8 | train loss: 0.6168\n",
      "[0.49452741766476926]\n",
      "Epoch:  9 | train loss: 0.6148\n",
      "[0.49452741766476926]\n",
      "Epoch:  10 | train loss: 0.6128\n",
      "[0.49452741766476926]\n",
      "Epoch:  11 | train loss: 0.6108\n",
      "[0.4940328902471045]\n",
      "Epoch:  12 | train loss: 0.6068\n",
      "[0.4940328902471045]\n",
      "Epoch:  13 | train loss: 0.6038\n",
      "[0.4935388573568574]\n",
      "Epoch:  14 | train loss: 0.5996\n",
      "[0.4935388573568574]\n",
      "Epoch:  15 | train loss: 0.5957\n",
      "[0.4935388573568574]\n",
      "Epoch:  16 | train loss: 0.5923\n",
      "[0.4930453184995005]\n",
      "Epoch:  17 | train loss: 0.5874\n",
      "[0.4930453184995005]\n",
      "Epoch:  18 | train loss: 0.5837\n",
      "[0.49255227318100103]\n",
      "Epoch:  19 | train loss: 0.5793\n",
      "[0.49255227318100103]\n",
      "Epoch:  20 | train loss: 0.5754\n",
      "[0.49255227318100103]\n",
      "Epoch:  21 | train loss: 0.5723\n",
      "[0.49205972090782]\n",
      "Epoch:  22 | train loss: 0.5677\n",
      "[0.49205972090782]\n",
      "Epoch:  23 | train loss: 0.5646\n",
      "[0.4915676611869122]\n",
      "Epoch:  24 | train loss: 0.5609\n",
      "[0.4915676611869122]\n",
      "Epoch:  25 | train loss: 0.5577\n",
      "[0.4915676611869122]\n",
      "Epoch:  26 | train loss: 0.5554\n",
      "[0.4910760935257253]\n",
      "Epoch:  27 | train loss: 0.5517\n",
      "[0.4910760935257253]\n",
      "Epoch:  28 | train loss: 0.5495\n",
      "[0.49058501743219957]\n",
      "Epoch:  29 | train loss: 0.5466\n",
      "[0.49058501743219957]\n",
      "Epoch:  30 | train loss: 0.5442\n",
      "[0.49058501743219957]\n",
      "Epoch:  31 | train loss: 0.5427\n",
      "[0.49009443241476736]\n",
      "Epoch:  32 | train loss: 0.5397\n",
      "[0.49009443241476736]\n",
      "Epoch:  33 | train loss: 0.5382\n",
      "[0.4896043379823526]\n",
      "Epoch:  34 | train loss: 0.5359\n",
      "[0.4896043379823526]\n",
      "Epoch:  35 | train loss: 0.5342\n",
      "[0.4896043379823526]\n",
      "Epoch:  36 | train loss: 0.5332\n",
      "[0.48911473364437025]\n",
      "Epoch:  37 | train loss: 0.5307\n",
      "[0.48911473364437025]\n",
      "Epoch:  38 | train loss: 0.5297\n",
      "[0.48862561891072587]\n",
      "Epoch:  39 | train loss: 0.5280\n",
      "[0.48862561891072587]\n",
      "Epoch:  40 | train loss: 0.5266\n",
      "[0.48862561891072587]\n",
      "Epoch:  41 | train loss: 0.5260\n",
      "[0.48813699329181515]\n",
      "Epoch:  42 | train loss: 0.5238\n",
      "[0.48813699329181515]\n",
      "Epoch:  43 | train loss: 0.5231\n",
      "[0.48764885629852334]\n",
      "Epoch:  44 | train loss: 0.5216\n",
      "[0.48764885629852334]\n",
      "Epoch:  45 | train loss: 0.5204\n",
      "[0.48764885629852334]\n",
      "Epoch:  46 | train loss: 0.5201\n",
      "[0.4871612074422248]\n",
      "Epoch:  47 | train loss: 0.5181\n",
      "[0.4871612074422248]\n",
      "Epoch:  48 | train loss: 0.5175\n",
      "[0.48667404623478255]\n",
      "Epoch:  49 | train loss: 0.5162\n",
      "[0.48667404623478255]\n",
      "Epoch:  50 | train loss: 0.5152\n",
      "[0.48667404623478255]\n",
      "Epoch:  51 | train loss: 0.5149\n",
      "[0.4861873721885478]\n",
      "Epoch:  52 | train loss: 0.5130\n",
      "[0.4861873721885478]\n",
      "Epoch:  53 | train loss: 0.5125\n",
      "[0.48570118481635927]\n",
      "Epoch:  54 | train loss: 0.5113\n",
      "[0.48570118481635927]\n",
      "Epoch:  55 | train loss: 0.5103\n",
      "[0.48570118481635927]\n",
      "Epoch:  56 | train loss: 0.5101\n",
      "[0.48521548363154293]\n",
      "Epoch:  57 | train loss: 0.5083\n",
      "[0.48521548363154293]\n",
      "Epoch:  58 | train loss: 0.5078\n",
      "[0.48473026814791137]\n",
      "Epoch:  59 | train loss: 0.5066\n",
      "[0.48473026814791137]\n",
      "Epoch:  60 | train loss: 0.5057\n",
      "[0.48473026814791137]\n",
      "Epoch:  61 | train loss: 0.5055\n",
      "[0.48424553787976343]\n",
      "Epoch:  62 | train loss: 0.5037\n",
      "[0.48424553787976343]\n",
      "Epoch:  63 | train loss: 0.5032\n",
      "[0.48376129234188364]\n",
      "Epoch:  64 | train loss: 0.5020\n",
      "[0.48376129234188364]\n",
      "Epoch:  65 | train loss: 0.5011\n",
      "[0.48376129234188364]\n",
      "Epoch:  66 | train loss: 0.5009\n",
      "[0.48327753104954174]\n",
      "Epoch:  67 | train loss: 0.4991\n",
      "[0.48327753104954174]\n",
      "Epoch:  68 | train loss: 0.4987\n",
      "[0.4827942535184922]\n",
      "Epoch:  69 | train loss: 0.4975\n",
      "[0.4827942535184922]\n",
      "Epoch:  70 | train loss: 0.4966\n",
      "[0.4827942535184922]\n",
      "Epoch:  71 | train loss: 0.4964\n",
      "[0.4823114592649737]\n",
      "Epoch:  72 | train loss: 0.4946\n",
      "[0.4823114592649737]\n",
      "Epoch:  73 | train loss: 0.4942\n",
      "[0.48182914780570874]\n",
      "Epoch:  74 | train loss: 0.4930\n",
      "[0.48182914780570874]\n",
      "Epoch:  75 | train loss: 0.4921\n",
      "[0.48182914780570874]\n",
      "Epoch:  76 | train loss: 0.4919\n",
      "[0.48134731865790303]\n",
      "Epoch:  77 | train loss: 0.4901\n",
      "[0.48134731865790303]\n",
      "Epoch:  78 | train loss: 0.4897\n",
      "[0.48086597133924514]\n",
      "Epoch:  79 | train loss: 0.4885\n",
      "[0.48086597133924514]\n",
      "Epoch:  80 | train loss: 0.4876\n",
      "[0.48086597133924514]\n",
      "Epoch:  81 | train loss: 0.4874\n",
      "[0.4803851053679059]\n",
      "Epoch:  82 | train loss: 0.4856\n",
      "[0.4803851053679059]\n",
      "Epoch:  83 | train loss: 0.4852\n",
      "[0.479904720262538]\n",
      "Epoch:  84 | train loss: 0.4840\n",
      "[0.479904720262538]\n",
      "Epoch:  85 | train loss: 0.4831\n",
      "[0.479904720262538]\n",
      "Epoch:  86 | train loss: 0.4829\n",
      "[0.47942481554227545]\n",
      "Epoch:  87 | train loss: 0.4811\n",
      "[0.47942481554227545]\n",
      "Epoch:  88 | train loss: 0.4807\n",
      "[0.4789453907267332]\n",
      "Epoch:  89 | train loss: 0.4795\n",
      "[0.4789453907267332]\n",
      "Epoch:  90 | train loss: 0.4786\n",
      "[0.4789453907267332]\n",
      "Epoch:  91 | train loss: 0.4784\n",
      "[0.47846644533600646]\n",
      "Epoch:  92 | train loss: 0.4766\n",
      "[0.47846644533600646]\n",
      "Epoch:  93 | train loss: 0.4762\n",
      "[0.47798797889067046]\n",
      "Epoch:  94 | train loss: 0.4751\n",
      "[0.47798797889067046]\n",
      "Epoch:  95 | train loss: 0.4741\n",
      "[0.47798797889067046]\n",
      "Epoch:  96 | train loss: 0.4740\n",
      "[0.4775099909117798]\n",
      "Epoch:  97 | train loss: 0.4722\n",
      "[0.4775099909117798]\n",
      "Epoch:  98 | train loss: 0.4718\n",
      "[0.47703248092086803]\n",
      "Epoch:  99 | train loss: 0.4706\n",
      "[0.47703248092086803]\n",
      "Epoch:  100 | train loss: 0.4697\n",
      "[0.47703248092086803]\n",
      "Epoch:  101 | train loss: 0.4695\n",
      "[0.4765554484399472]\n",
      "Epoch:  102 | train loss: 0.4678\n",
      "[0.4765554484399472]\n",
      "Epoch:  103 | train loss: 0.4673\n",
      "[0.47607889299150724]\n",
      "Epoch:  104 | train loss: 0.4662\n",
      "[0.47607889299150724]\n",
      "Epoch:  105 | train loss: 0.4653\n",
      "[0.47607889299150724]\n",
      "Epoch:  106 | train loss: 0.4651\n",
      "[0.47560281409851574]\n",
      "Epoch:  107 | train loss: 0.4634\n",
      "[0.47560281409851574]\n",
      "Epoch:  108 | train loss: 0.4629\n",
      "[0.4751272112844172]\n",
      "Epoch:  109 | train loss: 0.4618\n",
      "[0.4751272112844172]\n",
      "Epoch:  110 | train loss: 0.4609\n",
      "[0.4751272112844172]\n",
      "Epoch:  111 | train loss: 0.4607\n",
      "[0.4746520840731328]\n",
      "Epoch:  112 | train loss: 0.4590\n",
      "[0.4746520840731328]\n",
      "Epoch:  113 | train loss: 0.4586\n",
      "[0.47417743198905965]\n",
      "Epoch:  114 | train loss: 0.4574\n",
      "[0.47417743198905965]\n",
      "Epoch:  115 | train loss: 0.4565\n",
      "[0.47417743198905965]\n",
      "Epoch:  116 | train loss: 0.4564\n",
      "[0.4737032545570706]\n",
      "Epoch:  117 | train loss: 0.4546\n",
      "[0.4737032545570706]\n",
      "Epoch:  118 | train loss: 0.4542\n",
      "[0.4732295513025135]\n",
      "Epoch:  119 | train loss: 0.4531\n",
      "[0.4732295513025135]\n",
      "Epoch:  120 | train loss: 0.4522\n",
      "[0.4732295513025135]\n",
      "Epoch:  121 | train loss: 0.4521\n",
      "[0.47275632175121096]\n",
      "Epoch:  122 | train loss: 0.4503\n",
      "[0.47275632175121096]\n",
      "Epoch:  123 | train loss: 0.4499\n",
      "[0.47228356542945976]\n",
      "Epoch:  124 | train loss: 0.4488\n",
      "[0.47228356542945976]\n",
      "Epoch:  125 | train loss: 0.4479\n",
      "[0.47228356542945976]\n",
      "Epoch:  126 | train loss: 0.4478\n",
      "[0.4718112818640303]\n",
      "Epoch:  127 | train loss: 0.4461\n",
      "[0.4718112818640303]\n",
      "Epoch:  128 | train loss: 0.4456\n",
      "[0.4713394705821663]\n",
      "Epoch:  129 | train loss: 0.4446\n",
      "[0.4713394705821663]\n",
      "Epoch:  130 | train loss: 0.4437\n",
      "[0.4713394705821663]\n",
      "Epoch:  131 | train loss: 0.4435\n",
      "[0.47086813111158415]\n",
      "Epoch:  132 | train loss: 0.4418\n",
      "[0.47086813111158415]\n",
      "Epoch:  133 | train loss: 0.4414\n",
      "[0.4703972629804726]\n",
      "Epoch:  134 | train loss: 0.4403\n",
      "[0.4703972629804726]\n",
      "Epoch:  135 | train loss: 0.4394\n",
      "[0.4703972629804726]\n",
      "Epoch:  136 | train loss: 0.4393\n",
      "[0.4699268657174921]\n",
      "Epoch:  137 | train loss: 0.4376\n",
      "[0.4699268657174921]\n",
      "Epoch:  138 | train loss: 0.4372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4694569388517746]\n",
      "Epoch:  139 | train loss: 0.4361\n",
      "[0.4694569388517746]\n",
      "Epoch:  140 | train loss: 0.4352\n",
      "[0.4694569388517746]\n",
      "Epoch:  141 | train loss: 0.4351\n",
      "[0.46898748191292283]\n",
      "Epoch:  142 | train loss: 0.4334\n",
      "[0.46898748191292283]\n",
      "Epoch:  143 | train loss: 0.4330\n",
      "[0.4685184944310099]\n",
      "Epoch:  144 | train loss: 0.4320\n",
      "[0.4685184944310099]\n",
      "Epoch:  145 | train loss: 0.4311\n",
      "[0.4685184944310099]\n",
      "Epoch:  146 | train loss: 0.4310\n",
      "[0.4680499759365789]\n",
      "Epoch:  147 | train loss: 0.4293\n",
      "[0.4680499759365789]\n",
      "Epoch:  148 | train loss: 0.4289\n",
      "[0.4675819259606423]\n",
      "Epoch:  149 | train loss: 0.4278\n",
      "[0.4675819259606423]\n",
      "Epoch:  150 | train loss: 0.4269\n",
      "[0.4675819259606423]\n",
      "Epoch:  151 | train loss: 0.4268\n",
      "[0.46711434403468166]\n",
      "Epoch:  152 | train loss: 0.4252\n",
      "[0.46711434403468166]\n",
      "Epoch:  153 | train loss: 0.4248\n",
      "[0.466647229690647]\n",
      "Epoch:  154 | train loss: 0.4237\n",
      "[0.466647229690647]\n",
      "Epoch:  155 | train loss: 0.4229\n",
      "[0.466647229690647]\n",
      "Epoch:  156 | train loss: 0.4228\n",
      "[0.46618058246095634]\n",
      "Epoch:  157 | train loss: 0.4211\n",
      "[0.46618058246095634]\n",
      "Epoch:  158 | train loss: 0.4207\n",
      "[0.4657144018784954]\n",
      "Epoch:  159 | train loss: 0.4197\n",
      "[0.4657144018784954]\n",
      "Epoch:  160 | train loss: 0.4188\n",
      "[0.4657144018784954]\n",
      "Epoch:  161 | train loss: 0.4187\n",
      "[0.4652486874766169]\n",
      "Epoch:  162 | train loss: 0.4170\n",
      "[0.4652486874766169]\n",
      "Epoch:  163 | train loss: 0.4167\n",
      "[0.4647834387891403]\n",
      "Epoch:  164 | train loss: 0.4156\n",
      "[0.4647834387891403]\n",
      "Epoch:  165 | train loss: 0.4148\n",
      "[0.4647834387891403]\n",
      "Epoch:  166 | train loss: 0.4147\n",
      "[0.46431865535035116]\n",
      "Epoch:  167 | train loss: 0.4130\n",
      "[0.46431865535035116]\n",
      "Epoch:  168 | train loss: 0.4126\n",
      "[0.4638543366950008]\n",
      "Epoch:  169 | train loss: 0.4116\n",
      "[0.4638543366950008]\n",
      "Epoch:  170 | train loss: 0.4108\n",
      "[0.4638543366950008]\n",
      "Epoch:  171 | train loss: 0.4107\n",
      "[0.4633904823583058]\n",
      "Epoch:  172 | train loss: 0.4090\n",
      "[0.4633904823583058]\n",
      "Epoch:  173 | train loss: 0.4087\n",
      "[0.4629270918759475]\n",
      "Epoch:  174 | train loss: 0.4077\n",
      "[0.4629270918759475]\n",
      "Epoch:  175 | train loss: 0.4068\n",
      "[0.4629270918759475]\n",
      "Epoch:  176 | train loss: 0.4067\n",
      "[0.46246416478407154]\n",
      "Epoch:  177 | train loss: 0.4051\n",
      "[0.46246416478407154]\n",
      "Epoch:  178 | train loss: 0.4047\n",
      "[0.46200170061928747]\n",
      "Epoch:  179 | train loss: 0.4037\n",
      "[0.46200170061928747]\n",
      "Epoch:  180 | train loss: 0.4029\n",
      "[0.46200170061928747]\n",
      "Epoch:  181 | train loss: 0.4028\n",
      "[0.4615396989186682]\n",
      "Epoch:  182 | train loss: 0.4012\n",
      "[0.4615396989186682]\n",
      "Epoch:  183 | train loss: 0.4008\n",
      "[0.46107815921974954]\n",
      "Epoch:  184 | train loss: 0.3998\n",
      "[0.46107815921974954]\n",
      "Epoch:  185 | train loss: 0.3989\n",
      "[0.46107815921974954]\n",
      "Epoch:  186 | train loss: 0.3989\n",
      "[0.4606170810605298]\n",
      "Epoch:  187 | train loss: 0.3973\n",
      "[0.4606170810605298]\n",
      "Epoch:  188 | train loss: 0.3969\n",
      "[0.46015646397946924]\n",
      "Epoch:  189 | train loss: 0.3959\n",
      "[0.46015646397946924]\n",
      "Epoch:  190 | train loss: 0.3951\n",
      "[0.46015646397946924]\n",
      "Epoch:  191 | train loss: 0.3950\n",
      "[0.4596963075154898]\n",
      "Epoch:  192 | train loss: 0.3934\n",
      "[0.4596963075154898]\n",
      "Epoch:  193 | train loss: 0.3930\n",
      "[0.4592366112079743]\n",
      "Epoch:  194 | train loss: 0.3921\n",
      "[0.4592366112079743]\n",
      "Epoch:  195 | train loss: 0.3912\n",
      "[0.4592366112079743]\n",
      "Epoch:  196 | train loss: 0.3911\n",
      "[0.4587773745967664]\n",
      "Epoch:  197 | train loss: 0.3896\n",
      "[0.4587773745967664]\n",
      "Epoch:  198 | train loss: 0.3892\n",
      "[0.4583185972221696]\n",
      "Epoch:  199 | train loss: 0.3883\n",
      "[0.4583185972221696]\n",
      "Epoch:  200 | train loss: 0.3874\n",
      "[0.4583185972221696]\n",
      "Epoch:  201 | train loss: 0.3873\n",
      "[0.45786027862494744]\n",
      "Epoch:  202 | train loss: 0.3858\n",
      "[0.45786027862494744]\n",
      "Epoch:  203 | train loss: 0.3854\n",
      "[0.4574024183463225]\n",
      "Epoch:  204 | train loss: 0.3845\n",
      "[0.4574024183463225]\n",
      "Epoch:  205 | train loss: 0.3836\n",
      "[0.4574024183463225]\n",
      "Epoch:  206 | train loss: 0.3836\n",
      "[0.4569450159279762]\n",
      "Epoch:  207 | train loss: 0.3820\n",
      "[0.4569450159279762]\n",
      "Epoch:  208 | train loss: 0.3816\n",
      "[0.45648807091204824]\n",
      "Epoch:  209 | train loss: 0.3807\n",
      "[0.45648807091204824]\n",
      "Epoch:  210 | train loss: 0.3799\n",
      "[0.45648807091204824]\n",
      "Epoch:  211 | train loss: 0.3798\n",
      "[0.4560315828411362]\n",
      "Epoch:  212 | train loss: 0.3782\n",
      "[0.4560315828411362]\n",
      "Epoch:  213 | train loss: 0.3779\n",
      "[0.4555755512582951]\n",
      "Epoch:  214 | train loss: 0.3770\n",
      "[0.4555755512582951]\n",
      "Epoch:  215 | train loss: 0.3761\n",
      "[0.4555755512582951]\n",
      "Epoch:  216 | train loss: 0.3761\n",
      "[0.4551199757070368]\n",
      "Epoch:  217 | train loss: 0.3745\n",
      "[0.4551199757070368]\n",
      "Epoch:  218 | train loss: 0.3742\n",
      "[0.45466485573132975]\n",
      "Epoch:  219 | train loss: 0.3733\n",
      "[0.45466485573132975]\n",
      "Epoch:  220 | train loss: 0.3724\n",
      "[0.45466485573132975]\n",
      "Epoch:  221 | train loss: 0.3724\n",
      "[0.45421019087559844]\n",
      "Epoch:  222 | train loss: 0.3708\n",
      "[0.45421019087559844]\n",
      "Epoch:  223 | train loss: 0.3705\n",
      "[0.45375598068472284]\n",
      "Epoch:  224 | train loss: 0.3696\n",
      "[0.45375598068472284]\n",
      "Epoch:  225 | train loss: 0.3688\n",
      "[0.45375598068472284]\n",
      "Epoch:  226 | train loss: 0.3687\n",
      "[0.45330222470403814]\n",
      "Epoch:  227 | train loss: 0.3672\n",
      "[0.45330222470403814]\n",
      "Epoch:  228 | train loss: 0.3669\n",
      "[0.4528489224793341]\n",
      "Epoch:  229 | train loss: 0.3660\n",
      "[0.4528489224793341]\n",
      "Epoch:  230 | train loss: 0.3652\n",
      "[0.4528489224793341]\n",
      "Epoch:  231 | train loss: 0.3652\n",
      "[0.4523960735568548]\n",
      "Epoch:  232 | train loss: 0.3637\n",
      "[0.4523960735568548]\n",
      "Epoch:  233 | train loss: 0.3634\n",
      "[0.45194367748329795]\n",
      "Epoch:  234 | train loss: 0.3626\n",
      "[0.45194367748329795]\n",
      "Epoch:  235 | train loss: 0.3619\n",
      "[0.45194367748329795]\n",
      "Epoch:  236 | train loss: 0.3619\n",
      "[0.45149173380581464]\n",
      "Epoch:  237 | train loss: 0.3605\n",
      "[0.45149173380581464]\n",
      "Epoch:  238 | train loss: 0.3603\n",
      "[0.4510402420720088]\n",
      "Epoch:  239 | train loss: 0.3595\n",
      "[0.4510402420720088]\n",
      "Epoch:  240 | train loss: 0.3589\n",
      "[0.4510402420720088]\n",
      "Epoch:  241 | train loss: 0.3586\n",
      "[0.45058920182993684]\n",
      "Epoch:  242 | train loss: 0.3555\n",
      "[0.45058920182993684]\n",
      "Epoch:  243 | train loss: 0.3575\n",
      "[0.4501386126281069]\n",
      "Epoch:  244 | train loss: 0.3570\n",
      "[0.4501386126281069]\n",
      "Epoch:  245 | train loss: 0.3561\n",
      "[0.4501386126281069]\n",
      "Epoch:  246 | train loss: 0.3559\n",
      "[0.4496884740154788]\n",
      "Epoch:  247 | train loss: 0.3542\n",
      "[0.4496884740154788]\n",
      "Epoch:  248 | train loss: 0.3537\n",
      "[0.4492387855414633]\n",
      "Epoch:  249 | train loss: 0.3527\n",
      "[0.4492387855414633]\n",
      "Epoch:  250 | train loss: 0.3517\n",
      "[0.4492387855414633]\n",
      "Epoch:  251 | train loss: 0.3516\n",
      "[0.4487895467559218]\n",
      "Epoch:  252 | train loss: 0.3500\n",
      "[0.4487895467559218]\n",
      "Epoch:  253 | train loss: 0.3496\n",
      "[0.4483407572091659]\n",
      "Epoch:  254 | train loss: 0.3487\n",
      "[0.4483407572091659]\n",
      "Epoch:  255 | train loss: 0.3478\n",
      "[0.4483407572091659]\n",
      "Epoch:  256 | train loss: 0.3477\n",
      "[0.4478924164519567]\n",
      "Epoch:  257 | train loss: 0.3462\n",
      "[0.4478924164519567]\n",
      "Epoch:  258 | train loss: 0.3461\n",
      "[0.4474445240355048]\n",
      "Epoch:  259 | train loss: 0.3452\n",
      "[0.4474445240355048]\n",
      "Epoch:  260 | train loss: 0.3448\n",
      "[0.4474445240355048]\n",
      "Epoch:  261 | train loss: 0.3445\n",
      "[0.4469970795114693]\n",
      "Epoch:  262 | train loss: 0.3430\n",
      "[0.4469970795114693]\n",
      "Epoch:  263 | train loss: 0.3433\n",
      "[0.4465500824319578]\n",
      "Epoch:  264 | train loss: 0.3420\n",
      "[0.4465500824319578]\n",
      "Epoch:  265 | train loss: 0.3411\n",
      "[0.4465500824319578]\n",
      "Epoch:  266 | train loss: 0.3410\n",
      "[0.4461035323495259]\n",
      "Epoch:  267 | train loss: 0.3387\n",
      "[0.4461035323495259]\n",
      "Epoch:  268 | train loss: 0.3416\n",
      "[0.44565742881717635]\n",
      "Epoch:  269 | train loss: 0.3409\n",
      "[0.44565742881717635]\n",
      "Epoch:  270 | train loss: 0.3398\n",
      "[0.44565742881717635]\n",
      "Epoch:  271 | train loss: 0.3394\n",
      "[0.44521177138835916]\n",
      "Epoch:  272 | train loss: 0.3376\n",
      "[0.44521177138835916]\n",
      "Epoch:  273 | train loss: 0.3370\n",
      "[0.4447665596169708]\n",
      "Epoch:  274 | train loss: 0.3359\n",
      "[0.4447665596169708]\n",
      "Epoch:  275 | train loss: 0.3350\n",
      "[0.4447665596169708]\n",
      "Epoch:  276 | train loss: 0.3348\n",
      "[0.44432179305735386]\n",
      "Epoch:  277 | train loss: 0.3332\n",
      "[0.44432179305735386]\n",
      "Epoch:  278 | train loss: 0.3328\n",
      "[0.4438774712642965]\n",
      "Epoch:  279 | train loss: 0.3318\n",
      "[0.4438774712642965]\n",
      "Epoch:  280 | train loss: 0.3310\n",
      "[0.4438774712642965]\n",
      "Epoch:  281 | train loss: 0.3309\n",
      "[0.4434335937930322]\n",
      "Epoch:  282 | train loss: 0.3294\n",
      "[0.4434335937930322]\n",
      "Epoch:  283 | train loss: 0.3291\n",
      "[0.4429901601992392]\n",
      "Epoch:  284 | train loss: 0.3283\n",
      "[0.4429901601992392]\n",
      "Epoch:  285 | train loss: 0.3276\n",
      "[0.4429901601992392]\n",
      "Epoch:  286 | train loss: 0.3276\n",
      "[0.44254717003904]\n",
      "Epoch:  287 | train loss: 0.3263\n",
      "[0.44254717003904]\n",
      "Epoch:  288 | train loss: 0.3261\n",
      "[0.44210462286900093]\n",
      "Epoch:  289 | train loss: 0.3254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44210462286900093]\n",
      "Epoch:  290 | train loss: 0.3249\n",
      "[0.44210462286900093]\n",
      "Epoch:  291 | train loss: 0.3247\n",
      "[0.44166251824613195]\n",
      "Epoch:  292 | train loss: 0.3252\n",
      "[0.44166251824613195]\n",
      "Epoch:  293 | train loss: 0.3241\n",
      "[0.44122085572788583]\n",
      "Epoch:  294 | train loss: 0.3228\n",
      "[0.44122085572788583]\n",
      "Epoch:  295 | train loss: 0.3217\n",
      "[0.44122085572788583]\n",
      "Epoch:  296 | train loss: 0.3215\n",
      "[0.44077963487215793]\n",
      "Epoch:  297 | train loss: 0.3199\n",
      "[0.44077963487215793]\n",
      "Epoch:  298 | train loss: 0.3195\n",
      "[0.4403388552372858]\n",
      "Epoch:  299 | train loss: 0.3186\n",
      "[0.4403388552372858]\n",
      "Epoch:  300 | train loss: 0.3177\n",
      "[0.4403388552372858]\n",
      "Epoch:  301 | train loss: 0.3176\n",
      "[0.43989851638204847]\n",
      "Epoch:  302 | train loss: 0.3162\n",
      "[0.43989851638204847]\n",
      "Epoch:  303 | train loss: 0.3158\n",
      "[0.4394586178656664]\n",
      "Epoch:  304 | train loss: 0.3150\n",
      "[0.4394586178656664]\n",
      "Epoch:  305 | train loss: 0.3142\n",
      "[0.4394586178656664]\n",
      "Epoch:  306 | train loss: 0.3142\n",
      "[0.43901915924780077]\n",
      "Epoch:  307 | train loss: 0.3128\n",
      "[0.43901915924780077]\n",
      "Epoch:  308 | train loss: 0.3125\n",
      "[0.43858014008855295]\n",
      "Epoch:  309 | train loss: 0.3117\n",
      "[0.43858014008855295]\n",
      "Epoch:  310 | train loss: 0.3109\n",
      "[0.43858014008855295]\n",
      "Epoch:  311 | train loss: 0.3109\n",
      "[0.4381415599484644]\n",
      "Epoch:  312 | train loss: 0.3095\n",
      "[0.4381415599484644]\n",
      "Epoch:  313 | train loss: 0.3093\n",
      "[0.4377034183885159]\n",
      "Epoch:  314 | train loss: 0.3085\n",
      "[0.4377034183885159]\n",
      "Epoch:  315 | train loss: 0.3077\n",
      "[0.4377034183885159]\n",
      "Epoch:  316 | train loss: 0.3077\n",
      "[0.4372657149701274]\n",
      "Epoch:  317 | train loss: 0.3064\n",
      "[0.4372657149701274]\n",
      "Epoch:  318 | train loss: 0.3061\n",
      "[0.4368284492551573]\n",
      "Epoch:  319 | train loss: 0.3053\n",
      "[0.4368284492551573]\n",
      "Epoch:  320 | train loss: 0.3046\n",
      "[0.4368284492551573]\n",
      "Epoch:  321 | train loss: 0.3046\n",
      "[0.43639162080590216]\n",
      "Epoch:  322 | train loss: 0.3033\n",
      "[0.43639162080590216]\n",
      "Epoch:  323 | train loss: 0.3030\n",
      "[0.43595522918509627]\n",
      "Epoch:  324 | train loss: 0.3022\n",
      "[0.43595522918509627]\n",
      "Epoch:  325 | train loss: 0.3015\n",
      "[0.43595522918509627]\n",
      "Epoch:  326 | train loss: 0.3016\n",
      "[0.43551927395591117]\n",
      "Epoch:  327 | train loss: 0.3002\n",
      "[0.43551927395591117]\n",
      "Epoch:  328 | train loss: 0.3000\n",
      "[0.43508375468195526]\n",
      "Epoch:  329 | train loss: 0.2993\n",
      "[0.43508375468195526]\n",
      "Epoch:  330 | train loss: 0.2986\n",
      "[0.43508375468195526]\n",
      "Epoch:  331 | train loss: 0.2988\n",
      "[0.43464867092727333]\n",
      "Epoch:  332 | train loss: 0.2974\n",
      "[0.43464867092727333]\n",
      "Epoch:  333 | train loss: 0.2971\n",
      "[0.43421402225634603]\n",
      "Epoch:  334 | train loss: 0.2963\n",
      "[0.43421402225634603]\n",
      "Epoch:  335 | train loss: 0.2956\n",
      "[0.43421402225634603]\n",
      "Epoch:  336 | train loss: 0.2955\n",
      "[0.4337798082340897]\n",
      "Epoch:  337 | train loss: 0.2942\n",
      "[0.4337798082340897]\n",
      "Epoch:  338 | train loss: 0.2939\n",
      "[0.43334602842585557]\n",
      "Epoch:  339 | train loss: 0.2932\n",
      "[0.43334602842585557]\n",
      "Epoch:  340 | train loss: 0.2925\n",
      "[0.43334602842585557]\n",
      "Epoch:  341 | train loss: 0.2925\n",
      "[0.4329126823974297]\n",
      "Epoch:  342 | train loss: 0.2912\n",
      "[0.4329126823974297]\n",
      "Epoch:  343 | train loss: 0.2909\n",
      "[0.43247976971503227]\n",
      "Epoch:  344 | train loss: 0.2902\n",
      "[0.43247976971503227]\n",
      "Epoch:  345 | train loss: 0.2895\n",
      "[0.43247976971503227]\n",
      "Epoch:  346 | train loss: 0.2895\n",
      "[0.4320472899453172]\n",
      "Epoch:  347 | train loss: 0.2882\n",
      "[0.4320472899453172]\n",
      "Epoch:  348 | train loss: 0.2880\n",
      "[0.4316152426553719]\n",
      "Epoch:  349 | train loss: 0.2873\n",
      "[0.4316152426553719]\n",
      "Epoch:  350 | train loss: 0.2866\n",
      "[0.4316152426553719]\n",
      "Epoch:  351 | train loss: 0.2866\n",
      "[0.4311836274127165]\n",
      "Epoch:  352 | train loss: 0.2853\n",
      "[0.4311836274127165]\n",
      "Epoch:  353 | train loss: 0.2850\n",
      "[0.43075244378530375]\n",
      "Epoch:  354 | train loss: 0.2844\n",
      "[0.43075244378530375]\n",
      "Epoch:  355 | train loss: 0.2837\n",
      "[0.43075244378530375]\n",
      "Epoch:  356 | train loss: 0.2837\n",
      "[0.43032169134151843]\n",
      "Epoch:  357 | train loss: 0.2824\n",
      "[0.43032169134151843]\n",
      "Epoch:  358 | train loss: 0.2822\n",
      "[0.42989136965017694]\n",
      "Epoch:  359 | train loss: 0.2815\n",
      "[0.42989136965017694]\n",
      "Epoch:  360 | train loss: 0.2808\n",
      "[0.42989136965017694]\n",
      "Epoch:  361 | train loss: 0.2808\n",
      "[0.4294614782805268]\n",
      "Epoch:  362 | train loss: 0.2796\n",
      "[0.4294614782805268]\n",
      "Epoch:  363 | train loss: 0.2793\n",
      "[0.42903201680224623]\n",
      "Epoch:  364 | train loss: 0.2786\n",
      "[0.42903201680224623]\n",
      "Epoch:  365 | train loss: 0.2780\n",
      "[0.42903201680224623]\n",
      "Epoch:  366 | train loss: 0.2780\n",
      "[0.428602984785444]\n",
      "Epoch:  367 | train loss: 0.2767\n",
      "[0.428602984785444]\n",
      "Epoch:  368 | train loss: 0.2765\n",
      "[0.4281743818006586]\n",
      "Epoch:  369 | train loss: 0.2758\n",
      "[0.4281743818006586]\n",
      "Epoch:  370 | train loss: 0.2752\n",
      "[0.4281743818006586]\n",
      "Epoch:  371 | train loss: 0.2752\n",
      "[0.42774620741885794]\n",
      "Epoch:  372 | train loss: 0.2739\n",
      "[0.42774620741885794]\n",
      "Epoch:  373 | train loss: 0.2737\n",
      "[0.42731846121143907]\n",
      "Epoch:  374 | train loss: 0.2731\n",
      "[0.42731846121143907]\n",
      "Epoch:  375 | train loss: 0.2724\n",
      "[0.42731846121143907]\n",
      "Epoch:  376 | train loss: 0.2724\n",
      "[0.42689114275022766]\n",
      "Epoch:  377 | train loss: 0.2712\n",
      "[0.42689114275022766]\n",
      "Epoch:  378 | train loss: 0.2710\n",
      "[0.4264642516074774]\n",
      "Epoch:  379 | train loss: 0.2703\n",
      "[0.4264642516074774]\n",
      "Epoch:  380 | train loss: 0.2696\n",
      "[0.4264642516074774]\n",
      "Epoch:  381 | train loss: 0.2697\n",
      "[0.42603778735586995]\n",
      "Epoch:  382 | train loss: 0.2684\n",
      "[0.42603778735586995]\n",
      "Epoch:  383 | train loss: 0.2682\n",
      "[0.42561174956851405]\n",
      "Epoch:  384 | train loss: 0.2676\n",
      "[0.42561174956851405]\n",
      "Epoch:  385 | train loss: 0.2669\n",
      "[0.42561174956851405]\n",
      "Epoch:  386 | train loss: 0.2670\n",
      "[0.42518613781894554]\n",
      "Epoch:  387 | train loss: 0.2658\n",
      "[0.42518613781894554]\n",
      "Epoch:  388 | train loss: 0.2655\n",
      "[0.4247609516811266]\n",
      "Epoch:  389 | train loss: 0.2649\n",
      "[0.4247609516811266]\n",
      "Epoch:  390 | train loss: 0.2643\n",
      "[0.4247609516811266]\n",
      "Epoch:  391 | train loss: 0.2643\n",
      "[0.42433619072944545]\n",
      "Epoch:  392 | train loss: 0.2631\n",
      "[0.42433619072944545]\n",
      "Epoch:  393 | train loss: 0.2629\n",
      "[0.423911854538716]\n",
      "Epoch:  394 | train loss: 0.2622\n",
      "[0.423911854538716]\n",
      "Epoch:  395 | train loss: 0.2616\n",
      "[0.423911854538716]\n",
      "Epoch:  396 | train loss: 0.2616\n",
      "[0.4234879426841773]\n",
      "Epoch:  397 | train loss: 0.2604\n",
      "[0.4234879426841773]\n",
      "Epoch:  398 | train loss: 0.2602\n",
      "[0.4230644547414931]\n",
      "Epoch:  399 | train loss: 0.2596\n",
      "[0.4230644547414931]\n",
      "Epoch:  400 | train loss: 0.2590\n",
      "[0.4230644547414931]\n",
      "Epoch:  401 | train loss: 0.2590\n",
      "[0.42264139028675163]\n",
      "Epoch:  402 | train loss: 0.2578\n",
      "[0.42264139028675163]\n",
      "Epoch:  403 | train loss: 0.2576\n",
      "[0.4222187488964649]\n",
      "Epoch:  404 | train loss: 0.2570\n",
      "[0.4222187488964649]\n",
      "Epoch:  405 | train loss: 0.2564\n",
      "[0.4222187488964649]\n",
      "Epoch:  406 | train loss: 0.2564\n",
      "[0.4217965301475684]\n",
      "Epoch:  407 | train loss: 0.2553\n",
      "[0.4217965301475684]\n",
      "Epoch:  408 | train loss: 0.2551\n",
      "[0.42137473361742084]\n",
      "Epoch:  409 | train loss: 0.2545\n",
      "[0.42137473361742084]\n",
      "Epoch:  410 | train loss: 0.2538\n",
      "[0.42137473361742084]\n",
      "Epoch:  411 | train loss: 0.2539\n",
      "[0.4209533588838034]\n",
      "Epoch:  412 | train loss: 0.2527\n",
      "[0.4209533588838034]\n",
      "Epoch:  413 | train loss: 0.2525\n",
      "[0.4205324055249196]\n",
      "Epoch:  414 | train loss: 0.2519\n",
      "[0.4205324055249196]\n",
      "Epoch:  415 | train loss: 0.2513\n",
      "[0.4205324055249196]\n",
      "Epoch:  416 | train loss: 0.2513\n",
      "[0.4201118731193947]\n",
      "Epoch:  417 | train loss: 0.2502\n",
      "[0.4201118731193947]\n",
      "Epoch:  418 | train loss: 0.2500\n",
      "[0.4196917612462753]\n",
      "Epoch:  419 | train loss: 0.2494\n",
      "[0.4196917612462753]\n",
      "Epoch:  420 | train loss: 0.2488\n",
      "[0.4196917612462753]\n",
      "Epoch:  421 | train loss: 0.2489\n",
      "[0.419272069485029]\n",
      "Epoch:  422 | train loss: 0.2478\n",
      "[0.419272069485029]\n",
      "Epoch:  423 | train loss: 0.2476\n",
      "[0.41885279741554393]\n",
      "Epoch:  424 | train loss: 0.2470\n",
      "[0.41885279741554393]\n",
      "Epoch:  425 | train loss: 0.2465\n",
      "[0.41885279741554393]\n",
      "Epoch:  426 | train loss: 0.2465\n",
      "[0.4184339446181284]\n",
      "Epoch:  427 | train loss: 0.2453\n",
      "[0.4184339446181284]\n",
      "Epoch:  428 | train loss: 0.2451\n",
      "[0.4180155106735103]\n",
      "Epoch:  429 | train loss: 0.2445\n",
      "[0.4180155106735103]\n",
      "Epoch:  430 | train loss: 0.2439\n",
      "[0.4180155106735103]\n",
      "Epoch:  431 | train loss: 0.2440\n",
      "[0.4175974951628368]\n",
      "Epoch:  432 | train loss: 0.2429\n",
      "[0.4175974951628368]\n",
      "Epoch:  433 | train loss: 0.2427\n",
      "[0.41717989766767394]\n",
      "Epoch:  434 | train loss: 0.2421\n",
      "[0.41717989766767394]\n",
      "Epoch:  435 | train loss: 0.2415\n",
      "[0.41717989766767394]\n",
      "Epoch:  436 | train loss: 0.2416\n",
      "[0.4167627177700063]\n",
      "Epoch:  437 | train loss: 0.2405\n",
      "[0.4167627177700063]\n",
      "Epoch:  438 | train loss: 0.2403\n",
      "[0.41634595505223626]\n",
      "Epoch:  439 | train loss: 0.2397\n",
      "[0.41634595505223626]\n",
      "Epoch:  440 | train loss: 0.2391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41634595505223626]\n",
      "Epoch:  441 | train loss: 0.2392\n",
      "[0.415929609097184]\n",
      "Epoch:  442 | train loss: 0.2381\n",
      "[0.415929609097184]\n",
      "Epoch:  443 | train loss: 0.2379\n",
      "[0.4155136794880868]\n",
      "Epoch:  444 | train loss: 0.2374\n",
      "[0.4155136794880868]\n",
      "Epoch:  445 | train loss: 0.2368\n",
      "[0.4155136794880868]\n",
      "Epoch:  446 | train loss: 0.2368\n",
      "[0.4150981658085987]\n",
      "Epoch:  447 | train loss: 0.2358\n",
      "[0.4150981658085987]\n",
      "Epoch:  448 | train loss: 0.2356\n",
      "[0.4146830676427901]\n",
      "Epoch:  449 | train loss: 0.2351\n",
      "[0.4146830676427901]\n",
      "Epoch:  450 | train loss: 0.2345\n",
      "[0.4146830676427901]\n",
      "Epoch:  451 | train loss: 0.2345\n",
      "[0.4142683845751473]\n",
      "Epoch:  452 | train loss: 0.2335\n",
      "[0.4142683845751473]\n",
      "Epoch:  453 | train loss: 0.2333\n",
      "[0.41385411619057216]\n",
      "Epoch:  454 | train loss: 0.2328\n",
      "[0.41385411619057216]\n",
      "Epoch:  455 | train loss: 0.2322\n",
      "[0.41385411619057216]\n",
      "Epoch:  456 | train loss: 0.2323\n",
      "[0.4134402620743816]\n",
      "Epoch:  457 | train loss: 0.2312\n",
      "[0.4134402620743816]\n",
      "Epoch:  458 | train loss: 0.2310\n",
      "[0.4130268218123072]\n",
      "Epoch:  459 | train loss: 0.2305\n",
      "[0.4130268218123072]\n",
      "Epoch:  460 | train loss: 0.2300\n",
      "[0.4130268218123072]\n",
      "Epoch:  461 | train loss: 0.2300\n",
      "[0.4126137949904949]\n",
      "Epoch:  462 | train loss: 0.2290\n",
      "[0.4126137949904949]\n",
      "Epoch:  463 | train loss: 0.2288\n",
      "[0.4122011811955044]\n",
      "Epoch:  464 | train loss: 0.2283\n",
      "[0.4122011811955044]\n",
      "Epoch:  465 | train loss: 0.2277\n",
      "[0.4122011811955044]\n",
      "Epoch:  466 | train loss: 0.2278\n",
      "[0.4117889800143089]\n",
      "Epoch:  467 | train loss: 0.2267\n",
      "[0.4117889800143089]\n",
      "Epoch:  468 | train loss: 0.2266\n",
      "[0.4113771910342946]\n",
      "Epoch:  469 | train loss: 0.2261\n",
      "[0.4113771910342946]\n",
      "Epoch:  470 | train loss: 0.2255\n",
      "[0.4113771910342946]\n",
      "Epoch:  471 | train loss: 0.2256\n",
      "[0.4109658138432603]\n",
      "Epoch:  472 | train loss: 0.2246\n",
      "[0.4109658138432603]\n",
      "Epoch:  473 | train loss: 0.2244\n",
      "[0.41055484802941705]\n",
      "Epoch:  474 | train loss: 0.2239\n",
      "[0.41055484802941705]\n",
      "Epoch:  475 | train loss: 0.2234\n",
      "[0.41055484802941705]\n",
      "Epoch:  476 | train loss: 0.2234\n",
      "[0.4101442931813876]\n",
      "Epoch:  477 | train loss: 0.2224\n",
      "[0.4101442931813876]\n",
      "Epoch:  478 | train loss: 0.2223\n",
      "[0.40973414888820625]\n",
      "Epoch:  479 | train loss: 0.2218\n",
      "[0.40973414888820625]\n",
      "Epoch:  480 | train loss: 0.2212\n",
      "[0.40973414888820625]\n",
      "Epoch:  481 | train loss: 0.2213\n",
      "[0.409324414739318]\n",
      "Epoch:  482 | train loss: 0.2203\n",
      "[0.409324414739318]\n",
      "Epoch:  483 | train loss: 0.2201\n",
      "[0.4089150903245787]\n",
      "Epoch:  484 | train loss: 0.2197\n",
      "[0.4089150903245787]\n",
      "Epoch:  485 | train loss: 0.2191\n",
      "[0.4089150903245787]\n",
      "Epoch:  486 | train loss: 0.2192\n",
      "[0.4085061752342541]\n",
      "Epoch:  487 | train loss: 0.2182\n",
      "[0.4085061752342541]\n",
      "Epoch:  488 | train loss: 0.2181\n",
      "[0.40809766905901984]\n",
      "Epoch:  489 | train loss: 0.2176\n",
      "[0.40809766905901984]\n",
      "Epoch:  490 | train loss: 0.2171\n",
      "[0.40809766905901984]\n",
      "Epoch:  491 | train loss: 0.2171\n",
      "[0.4076895713899608]\n",
      "Epoch:  492 | train loss: 0.2161\n",
      "[0.4076895713899608]\n",
      "Epoch:  493 | train loss: 0.2160\n",
      "[0.40728188181857083]\n",
      "Epoch:  494 | train loss: 0.2155\n",
      "[0.40728188181857083]\n",
      "Epoch:  495 | train loss: 0.2150\n",
      "[0.40728188181857083]\n",
      "Epoch:  496 | train loss: 0.2151\n",
      "[0.4068745999367523]\n",
      "Epoch:  497 | train loss: 0.2141\n",
      "[0.4068745999367523]\n",
      "Epoch:  498 | train loss: 0.2140\n",
      "[0.4064677253368155]\n",
      "Epoch:  499 | train loss: 0.2135\n",
      "[0.4064677253368155]\n",
      "Epoch:  500 | train loss: 0.2130\n",
      "[0.4064677253368155]\n",
      "Epoch:  501 | train loss: 0.2131\n",
      "[0.4060612576114787]\n",
      "Epoch:  502 | train loss: 0.2121\n",
      "[0.4060612576114787]\n",
      "Epoch:  503 | train loss: 0.2120\n",
      "[0.4056551963538672]\n",
      "Epoch:  504 | train loss: 0.2115\n",
      "[0.4056551963538672]\n",
      "Epoch:  505 | train loss: 0.2110\n",
      "[0.4056551963538672]\n",
      "Epoch:  506 | train loss: 0.2111\n",
      "[0.4052495411575133]\n",
      "Epoch:  507 | train loss: 0.2101\n",
      "[0.4052495411575133]\n",
      "Epoch:  508 | train loss: 0.2100\n",
      "[0.4048442916163558]\n",
      "Epoch:  509 | train loss: 0.2095\n",
      "[0.4048442916163558]\n",
      "Epoch:  510 | train loss: 0.2090\n",
      "[0.4048442916163558]\n",
      "Epoch:  511 | train loss: 0.2091\n",
      "[0.40443944732473947]\n",
      "Epoch:  512 | train loss: 0.2082\n",
      "[0.40443944732473947]\n",
      "Epoch:  513 | train loss: 0.2080\n",
      "[0.4040350078774147]\n",
      "Epoch:  514 | train loss: 0.2076\n",
      "[0.4040350078774147]\n",
      "Epoch:  515 | train loss: 0.2071\n",
      "[0.4040350078774147]\n",
      "Epoch:  516 | train loss: 0.2072\n",
      "[0.4036309728695373]\n",
      "Epoch:  517 | train loss: 0.2062\n",
      "[0.4036309728695373]\n",
      "Epoch:  518 | train loss: 0.2061\n",
      "[0.40322734189666776]\n",
      "Epoch:  519 | train loss: 0.2057\n",
      "[0.40322734189666776]\n",
      "Epoch:  520 | train loss: 0.2052\n",
      "[0.40322734189666776]\n",
      "Epoch:  521 | train loss: 0.2053\n",
      "[0.4028241145547711]\n",
      "Epoch:  522 | train loss: 0.2043\n",
      "[0.4028241145547711]\n",
      "Epoch:  523 | train loss: 0.2042\n",
      "[0.4024212904402163]\n",
      "Epoch:  524 | train loss: 0.2038\n",
      "[0.4024212904402163]\n",
      "Epoch:  525 | train loss: 0.2033\n",
      "[0.4024212904402163]\n",
      "Epoch:  526 | train loss: 0.2034\n",
      "[0.4020188691497761]\n",
      "Epoch:  527 | train loss: 0.2025\n",
      "[0.4020188691497761]\n",
      "Epoch:  528 | train loss: 0.2024\n",
      "[0.4016168502806263]\n",
      "Epoch:  529 | train loss: 0.2019\n",
      "[0.4016168502806263]\n",
      "Epoch:  530 | train loss: 0.2015\n",
      "[0.4016168502806263]\n",
      "Epoch:  531 | train loss: 0.2016\n",
      "[0.4012152334303457]\n",
      "Epoch:  532 | train loss: 0.2007\n",
      "[0.4012152334303457]\n",
      "Epoch:  533 | train loss: 0.2005\n",
      "[0.40081401819691537]\n",
      "Epoch:  534 | train loss: 0.2001\n",
      "[0.40081401819691537]\n",
      "Epoch:  535 | train loss: 0.1996\n",
      "[0.40081401819691537]\n",
      "Epoch:  536 | train loss: 0.1997\n",
      "[0.40041320417871845]\n",
      "Epoch:  537 | train loss: 0.1988\n",
      "[0.40041320417871845]\n",
      "Epoch:  538 | train loss: 0.1987\n",
      "[0.40001279097453973]\n",
      "Epoch:  539 | train loss: 0.1983\n",
      "[0.40001279097453973]\n",
      "Epoch:  540 | train loss: 0.1978\n",
      "[0.40001279097453973]\n",
      "Epoch:  541 | train loss: 0.1979\n",
      "[0.3996127781835652]\n",
      "Epoch:  542 | train loss: 0.1970\n",
      "[0.3996127781835652]\n",
      "Epoch:  543 | train loss: 0.1969\n",
      "[0.39921316540538165]\n",
      "Epoch:  544 | train loss: 0.1965\n",
      "[0.39921316540538165]\n",
      "Epoch:  545 | train loss: 0.1961\n",
      "[0.39921316540538165]\n",
      "Epoch:  546 | train loss: 0.1961\n",
      "[0.3988139522399763]\n",
      "Epoch:  547 | train loss: 0.1953\n",
      "[0.3988139522399763]\n",
      "Epoch:  548 | train loss: 0.1952\n",
      "[0.39841513828773634]\n",
      "Epoch:  549 | train loss: 0.1948\n",
      "[0.39841513828773634]\n",
      "Epoch:  550 | train loss: 0.1943\n",
      "[0.39841513828773634]\n",
      "Epoch:  551 | train loss: 0.1944\n",
      "[0.3980167231494486]\n",
      "Epoch:  552 | train loss: 0.1935\n",
      "[0.3980167231494486]\n",
      "Epoch:  553 | train loss: 0.1934\n",
      "[0.39761870642629915]\n",
      "Epoch:  554 | train loss: 0.1930\n",
      "[0.39761870642629915]\n",
      "Epoch:  555 | train loss: 0.1926\n",
      "[0.39761870642629915]\n",
      "Epoch:  556 | train loss: 0.1927\n",
      "[0.39722108771987286]\n",
      "Epoch:  557 | train loss: 0.1918\n",
      "[0.39722108771987286]\n",
      "Epoch:  558 | train loss: 0.1917\n",
      "[0.39682386663215297]\n",
      "Epoch:  559 | train loss: 0.1913\n",
      "[0.39682386663215297]\n",
      "Epoch:  560 | train loss: 0.1909\n",
      "[0.39682386663215297]\n",
      "Epoch:  561 | train loss: 0.1910\n",
      "[0.3964270427655208]\n",
      "Epoch:  562 | train loss: 0.1901\n",
      "[0.3964270427655208]\n",
      "Epoch:  563 | train loss: 0.1901\n",
      "[0.3960306157227553]\n",
      "Epoch:  564 | train loss: 0.1897\n",
      "[0.3960306157227553]\n",
      "Epoch:  565 | train loss: 0.1892\n",
      "[0.3960306157227553]\n",
      "Epoch:  566 | train loss: 0.1893\n",
      "[0.39563458510703253]\n",
      "Epoch:  567 | train loss: 0.1885\n",
      "[0.39563458510703253]\n",
      "Epoch:  568 | train loss: 0.1884\n",
      "[0.3952389505219255]\n",
      "Epoch:  569 | train loss: 0.1880\n",
      "[0.3952389505219255]\n",
      "Epoch:  570 | train loss: 0.1876\n",
      "[0.3952389505219255]\n",
      "Epoch:  571 | train loss: 0.1877\n",
      "[0.3948437115714036]\n",
      "Epoch:  572 | train loss: 0.1868\n",
      "[0.3948437115714036]\n",
      "Epoch:  573 | train loss: 0.1868\n",
      "[0.3944488678598322]\n",
      "Epoch:  574 | train loss: 0.1864\n",
      "[0.3944488678598322]\n",
      "Epoch:  575 | train loss: 0.1860\n",
      "[0.3944488678598322]\n",
      "Epoch:  576 | train loss: 0.1861\n",
      "[0.39405441899197236]\n",
      "Epoch:  577 | train loss: 0.1852\n",
      "[0.39405441899197236]\n",
      "Epoch:  578 | train loss: 0.1852\n",
      "[0.39366036457298037]\n",
      "Epoch:  579 | train loss: 0.1848\n",
      "[0.39366036457298037]\n",
      "Epoch:  580 | train loss: 0.1844\n",
      "[0.39366036457298037]\n",
      "Epoch:  581 | train loss: 0.1845\n",
      "[0.39326670420840737]\n",
      "Epoch:  582 | train loss: 0.1837\n",
      "[0.39326670420840737]\n",
      "Epoch:  583 | train loss: 0.1836\n",
      "[0.39287343750419895]\n",
      "Epoch:  584 | train loss: 0.1832\n",
      "[0.39287343750419895]\n",
      "Epoch:  585 | train loss: 0.1828\n",
      "[0.39287343750419895]\n",
      "Epoch:  586 | train loss: 0.1829\n",
      "[0.39248056406669474]\n",
      "Epoch:  587 | train loss: 0.1821\n",
      "[0.39248056406669474]\n",
      "Epoch:  588 | train loss: 0.1820\n",
      "[0.392088083502628]\n",
      "Epoch:  589 | train loss: 0.1817\n",
      "[0.392088083502628]\n",
      "Epoch:  590 | train loss: 0.1813\n",
      "[0.392088083502628]\n",
      "Epoch:  591 | train loss: 0.1814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3916959954191254]\n",
      "Epoch:  592 | train loss: 0.1806\n",
      "[0.3916959954191254]\n",
      "Epoch:  593 | train loss: 0.1805\n",
      "[0.3913042994237063]\n",
      "Epoch:  594 | train loss: 0.1802\n",
      "[0.3913042994237063]\n",
      "Epoch:  595 | train loss: 0.1798\n",
      "[0.3913042994237063]\n",
      "Epoch:  596 | train loss: 0.1799\n",
      "[0.3909129951242826]\n",
      "Epoch:  597 | train loss: 0.1791\n",
      "[0.3909129951242826]\n",
      "Epoch:  598 | train loss: 0.1790\n",
      "[0.39052208212915834]\n",
      "Epoch:  599 | train loss: 0.1787\n",
      "[0.39052208212915834]\n",
      "Epoch:  600 | train loss: 0.1783\n",
      "[0.39052208212915834]\n",
      "Epoch:  601 | train loss: 0.1784\n",
      "[0.3901315600470292]\n",
      "Epoch:  602 | train loss: 0.1776\n",
      "[0.3901315600470292]\n",
      "Epoch:  603 | train loss: 0.1775\n",
      "[0.3897414284869822]\n",
      "Epoch:  604 | train loss: 0.1772\n",
      "[0.3897414284869822]\n",
      "Epoch:  605 | train loss: 0.1768\n",
      "[0.3897414284869822]\n",
      "Epoch:  606 | train loss: 0.1769\n",
      "[0.3893516870584952]\n",
      "Epoch:  607 | train loss: 0.1761\n",
      "[0.3893516870584952]\n",
      "Epoch:  608 | train loss: 0.1761\n",
      "[0.3889623353714367]\n",
      "Epoch:  609 | train loss: 0.1757\n",
      "[0.3889623353714367]\n",
      "Epoch:  610 | train loss: 0.1753\n",
      "[0.3889623353714367]\n",
      "Epoch:  611 | train loss: 0.1755\n",
      "[0.38857337303606526]\n",
      "Epoch:  612 | train loss: 0.1747\n",
      "[0.38857337303606526]\n",
      "Epoch:  613 | train loss: 0.1746\n",
      "[0.3881847996630292]\n",
      "Epoch:  614 | train loss: 0.1743\n",
      "[0.3881847996630292]\n",
      "Epoch:  615 | train loss: 0.1739\n",
      "[0.3881847996630292]\n",
      "Epoch:  616 | train loss: 0.1740\n",
      "[0.3877966148633662]\n",
      "Epoch:  617 | train loss: 0.1733\n",
      "[0.3877966148633662]\n",
      "Epoch:  618 | train loss: 0.1732\n",
      "[0.3874088182485028]\n",
      "Epoch:  619 | train loss: 0.1729\n",
      "[0.3874088182485028]\n",
      "Epoch:  620 | train loss: 0.1725\n",
      "[0.3874088182485028]\n",
      "Epoch:  621 | train loss: 0.1726\n",
      "[0.3870214094302543]\n",
      "Epoch:  622 | train loss: 0.1719\n",
      "[0.3870214094302543]\n",
      "Epoch:  623 | train loss: 0.1718\n",
      "[0.38663438802082406]\n",
      "Epoch:  624 | train loss: 0.1715\n",
      "[0.38663438802082406]\n",
      "Epoch:  625 | train loss: 0.1711\n",
      "[0.38663438802082406]\n",
      "Epoch:  626 | train loss: 0.1712\n",
      "[0.3862477536328032]\n",
      "Epoch:  627 | train loss: 0.1705\n",
      "[0.3862477536328032]\n",
      "Epoch:  628 | train loss: 0.1705\n",
      "[0.3858615058791704]\n",
      "Epoch:  629 | train loss: 0.1701\n",
      "[0.3858615058791704]\n",
      "Epoch:  630 | train loss: 0.1698\n",
      "[0.3858615058791704]\n",
      "Epoch:  631 | train loss: 0.1699\n",
      "[0.38547564437329124]\n",
      "Epoch:  632 | train loss: 0.1691\n",
      "[0.38547564437329124]\n",
      "Epoch:  633 | train loss: 0.1691\n",
      "[0.38509016872891794]\n",
      "Epoch:  634 | train loss: 0.1688\n",
      "[0.38509016872891794]\n",
      "Epoch:  635 | train loss: 0.1684\n",
      "[0.38509016872891794]\n",
      "Epoch:  636 | train loss: 0.1686\n",
      "[0.38470507856018904]\n",
      "Epoch:  637 | train loss: 0.1678\n",
      "[0.38470507856018904]\n",
      "Epoch:  638 | train loss: 0.1678\n",
      "[0.38432037348162884]\n",
      "Epoch:  639 | train loss: 0.1675\n",
      "[0.38432037348162884]\n",
      "Epoch:  640 | train loss: 0.1671\n",
      "[0.38432037348162884]\n",
      "Epoch:  641 | train loss: 0.1672\n",
      "[0.3839360531081472]\n",
      "Epoch:  642 | train loss: 0.1665\n",
      "[0.3839360531081472]\n",
      "Epoch:  643 | train loss: 0.1665\n",
      "[0.38355211705503905]\n",
      "Epoch:  644 | train loss: 0.1662\n",
      "[0.38355211705503905]\n",
      "Epoch:  645 | train loss: 0.1658\n",
      "[0.38355211705503905]\n",
      "Epoch:  646 | train loss: 0.1660\n",
      "[0.383168564937984]\n",
      "Epoch:  647 | train loss: 0.1652\n",
      "[0.383168564937984]\n",
      "Epoch:  648 | train loss: 0.1652\n",
      "[0.38278539637304604]\n",
      "Epoch:  649 | train loss: 0.1649\n",
      "[0.38278539637304604]\n",
      "Epoch:  650 | train loss: 0.1646\n",
      "[0.38278539637304604]\n",
      "Epoch:  651 | train loss: 0.1647\n",
      "[0.382402610976673]\n",
      "Epoch:  652 | train loss: 0.1640\n",
      "[0.382402610976673]\n",
      "Epoch:  653 | train loss: 0.1640\n",
      "[0.38202020836569633]\n",
      "Epoch:  654 | train loss: 0.1637\n",
      "[0.38202020836569633]\n",
      "Epoch:  655 | train loss: 0.1633\n",
      "[0.38202020836569633]\n",
      "Epoch:  656 | train loss: 0.1634\n",
      "[0.38163818815733064]\n",
      "Epoch:  657 | train loss: 0.1627\n",
      "[0.38163818815733064]\n",
      "Epoch:  658 | train loss: 0.1627\n",
      "[0.3812565499691733]\n",
      "Epoch:  659 | train loss: 0.1624\n",
      "[0.3812565499691733]\n",
      "Epoch:  660 | train loss: 0.1621\n",
      "[0.3812565499691733]\n",
      "Epoch:  661 | train loss: 0.1622\n",
      "[0.3808752934192041]\n",
      "Epoch:  662 | train loss: 0.1615\n",
      "[0.3808752934192041]\n",
      "Epoch:  663 | train loss: 0.1615\n",
      "[0.38049441812578494]\n",
      "Epoch:  664 | train loss: 0.1612\n",
      "[0.38049441812578494]\n",
      "Epoch:  665 | train loss: 0.1609\n",
      "[0.38049441812578494]\n",
      "Epoch:  666 | train loss: 0.1610\n",
      "[0.3801139237076592]\n",
      "Epoch:  667 | train loss: 0.1603\n",
      "[0.3801139237076592]\n",
      "Epoch:  668 | train loss: 0.1603\n",
      "[0.3797338097839515]\n",
      "Epoch:  669 | train loss: 0.1600\n",
      "[0.3797338097839515]\n",
      "Epoch:  670 | train loss: 0.1597\n",
      "[0.3797338097839515]\n",
      "Epoch:  671 | train loss: 0.1598\n",
      "[0.37935407597416754]\n",
      "Epoch:  672 | train loss: 0.1591\n",
      "[0.37935407597416754]\n",
      "Epoch:  673 | train loss: 0.1591\n",
      "[0.37897472189819337]\n",
      "Epoch:  674 | train loss: 0.1588\n",
      "[0.37897472189819337]\n",
      "Epoch:  675 | train loss: 0.1585\n",
      "[0.37897472189819337]\n",
      "Epoch:  676 | train loss: 0.1586\n",
      "[0.3785957471762952]\n",
      "Epoch:  677 | train loss: 0.1580\n",
      "[0.3785957471762952]\n",
      "Epoch:  678 | train loss: 0.1580\n",
      "[0.3782171514291189]\n",
      "Epoch:  679 | train loss: 0.1577\n",
      "[0.3782171514291189]\n",
      "Epoch:  680 | train loss: 0.1574\n",
      "[0.3782171514291189]\n",
      "Epoch:  681 | train loss: 0.1575\n",
      "[0.3778389342776898]\n",
      "Epoch:  682 | train loss: 0.1568\n",
      "[0.3778389342776898]\n",
      "Epoch:  683 | train loss: 0.1568\n",
      "[0.3774610953434121]\n",
      "Epoch:  684 | train loss: 0.1566\n",
      "[0.3774610953434121]\n",
      "Epoch:  685 | train loss: 0.1562\n",
      "[0.3774610953434121]\n",
      "Epoch:  686 | train loss: 0.1564\n",
      "[0.3770836342480687]\n",
      "Epoch:  687 | train loss: 0.1557\n",
      "[0.3770836342480687]\n",
      "Epoch:  688 | train loss: 0.1557\n",
      "[0.37670655061382063]\n",
      "Epoch:  689 | train loss: 0.1554\n",
      "[0.37670655061382063]\n",
      "Epoch:  690 | train loss: 0.1551\n",
      "[0.37670655061382063]\n",
      "Epoch:  691 | train loss: 0.1553\n",
      "[0.3763298440632068]\n",
      "Epoch:  692 | train loss: 0.1546\n",
      "[0.3763298440632068]\n",
      "Epoch:  693 | train loss: 0.1546\n",
      "[0.3759535142191436]\n",
      "Epoch:  694 | train loss: 0.1543\n",
      "[0.3759535142191436]\n",
      "Epoch:  695 | train loss: 0.1540\n",
      "[0.3759535142191436]\n",
      "Epoch:  696 | train loss: 0.1542\n",
      "[0.37557756070492443]\n",
      "Epoch:  697 | train loss: 0.1535\n",
      "[0.37557756070492443]\n",
      "Epoch:  698 | train loss: 0.1535\n",
      "[0.3752019831442195]\n",
      "Epoch:  699 | train loss: 0.1533\n",
      "[0.3752019831442195]\n",
      "Epoch:  700 | train loss: 0.1530\n",
      "[0.3752019831442195]\n",
      "Epoch:  701 | train loss: 0.1531\n",
      "[0.37482678116107526]\n",
      "Epoch:  702 | train loss: 0.1524\n",
      "[0.37482678116107526]\n",
      "Epoch:  703 | train loss: 0.1525\n",
      "[0.3744519543799142]\n",
      "Epoch:  704 | train loss: 0.1522\n",
      "[0.3744519543799142]\n",
      "Epoch:  705 | train loss: 0.1519\n",
      "[0.3744519543799142]\n",
      "Epoch:  706 | train loss: 0.1520\n",
      "[0.3740775024255343]\n",
      "Epoch:  707 | train loss: 0.1514\n",
      "[0.3740775024255343]\n",
      "Epoch:  708 | train loss: 0.1514\n",
      "[0.37370342492310876]\n",
      "Epoch:  709 | train loss: 0.1512\n",
      "[0.37370342492310876]\n",
      "Epoch:  710 | train loss: 0.1509\n",
      "[0.37370342492310876]\n",
      "Epoch:  711 | train loss: 0.1510\n",
      "[0.37332972149818566]\n",
      "Epoch:  712 | train loss: 0.1504\n",
      "[0.37332972149818566]\n",
      "Epoch:  713 | train loss: 0.1504\n",
      "[0.3729563917766875]\n",
      "Epoch:  714 | train loss: 0.1501\n",
      "[0.3729563917766875]\n",
      "Epoch:  715 | train loss: 0.1499\n",
      "[0.3729563917766875]\n",
      "Epoch:  716 | train loss: 0.1500\n",
      "[0.3725834353849108]\n",
      "Epoch:  717 | train loss: 0.1493\n",
      "[0.3725834353849108]\n",
      "Epoch:  718 | train loss: 0.1494\n",
      "[0.37221085194952586]\n",
      "Epoch:  719 | train loss: 0.1491\n",
      "[0.37221085194952586]\n",
      "Epoch:  720 | train loss: 0.1489\n",
      "[0.37221085194952586]\n",
      "Epoch:  721 | train loss: 0.1490\n",
      "[0.3718386410975763]\n",
      "Epoch:  722 | train loss: 0.1484\n",
      "[0.3718386410975763]\n",
      "Epoch:  723 | train loss: 0.1484\n",
      "[0.37146680245647873]\n",
      "Epoch:  724 | train loss: 0.1481\n",
      "[0.37146680245647873]\n",
      "Epoch:  725 | train loss: 0.1479\n",
      "[0.37146680245647873]\n",
      "Epoch:  726 | train loss: 0.1480\n",
      "[0.37109533565402225]\n",
      "Epoch:  727 | train loss: 0.1474\n",
      "[0.37109533565402225]\n",
      "Epoch:  728 | train loss: 0.1474\n",
      "[0.3707242403183682]\n",
      "Epoch:  729 | train loss: 0.1472\n",
      "[0.3707242403183682]\n",
      "Epoch:  730 | train loss: 0.1469\n",
      "[0.3707242403183682]\n",
      "Epoch:  731 | train loss: 0.1470\n",
      "[0.37035351607804984]\n",
      "Epoch:  732 | train loss: 0.1464\n",
      "[0.37035351607804984]\n",
      "Epoch:  733 | train loss: 0.1464\n",
      "[0.3699831625619718]\n",
      "Epoch:  734 | train loss: 0.1462\n",
      "[0.3699831625619718]\n",
      "Epoch:  735 | train loss: 0.1459\n",
      "[0.3699831625619718]\n",
      "Epoch:  736 | train loss: 0.1461\n",
      "[0.3696131793994098]\n",
      "Epoch:  737 | train loss: 0.1455\n",
      "[0.3696131793994098]\n",
      "Epoch:  738 | train loss: 0.1455\n",
      "[0.36924356622001037]\n",
      "Epoch:  739 | train loss: 0.1453\n",
      "[0.36924356622001037]\n",
      "Epoch:  740 | train loss: 0.1450\n",
      "[0.36924356622001037]\n",
      "Epoch:  741 | train loss: 0.1451\n",
      "[0.36887432265379033]\n",
      "Epoch:  742 | train loss: 0.1445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36887432265379033]\n",
      "Epoch:  743 | train loss: 0.1446\n",
      "[0.3685054483311365]\n",
      "Epoch:  744 | train loss: 0.1444\n",
      "[0.3685054483311365]\n",
      "Epoch:  745 | train loss: 0.1441\n",
      "[0.3685054483311365]\n",
      "Epoch:  746 | train loss: 0.1442\n",
      "[0.3681369428828054]\n",
      "Epoch:  747 | train loss: 0.1436\n",
      "[0.3681369428828054]\n",
      "Epoch:  748 | train loss: 0.1437\n",
      "[0.3677688059399226]\n",
      "Epoch:  749 | train loss: 0.1434\n",
      "[0.3677688059399226]\n",
      "Epoch:  750 | train loss: 0.1432\n",
      "[0.3677688059399226]\n",
      "Epoch:  751 | train loss: 0.1433\n",
      "[0.3674010371339827]\n",
      "Epoch:  752 | train loss: 0.1427\n",
      "[0.3674010371339827]\n",
      "Epoch:  753 | train loss: 0.1428\n",
      "[0.3670336360968487]\n",
      "Epoch:  754 | train loss: 0.1426\n",
      "[0.3670336360968487]\n",
      "Epoch:  755 | train loss: 0.1423\n",
      "[0.3670336360968487]\n",
      "Epoch:  756 | train loss: 0.1424\n",
      "[0.36666660246075183]\n",
      "Epoch:  757 | train loss: 0.1418\n",
      "[0.36666660246075183]\n",
      "Epoch:  758 | train loss: 0.1419\n",
      "[0.36629993585829107]\n",
      "Epoch:  759 | train loss: 0.1417\n",
      "[0.36629993585829107]\n",
      "Epoch:  760 | train loss: 0.1414\n",
      "[0.36629993585829107]\n",
      "Epoch:  761 | train loss: 0.1416\n",
      "[0.3659336359224328]\n",
      "Epoch:  762 | train loss: 0.1410\n",
      "[0.3659336359224328]\n",
      "Epoch:  763 | train loss: 0.1410\n",
      "[0.36556770228651037]\n",
      "Epoch:  764 | train loss: 0.1408\n",
      "[0.36556770228651037]\n",
      "Epoch:  765 | train loss: 0.1406\n",
      "[0.36556770228651037]\n",
      "Epoch:  766 | train loss: 0.1407\n",
      "[0.36520213458422385]\n",
      "Epoch:  767 | train loss: 0.1401\n",
      "[0.36520213458422385]\n",
      "Epoch:  768 | train loss: 0.1402\n",
      "[0.3648369324496396]\n",
      "Epoch:  769 | train loss: 0.1400\n",
      "[0.3648369324496396]\n",
      "Epoch:  770 | train loss: 0.1397\n",
      "[0.3648369324496396]\n",
      "Epoch:  771 | train loss: 0.1399\n",
      "[0.36447209551718995]\n",
      "Epoch:  772 | train loss: 0.1393\n",
      "[0.36447209551718995]\n",
      "Epoch:  773 | train loss: 0.1393\n",
      "[0.36410762342167274]\n",
      "Epoch:  774 | train loss: 0.1391\n",
      "[0.36410762342167274]\n",
      "Epoch:  775 | train loss: 0.1389\n",
      "[0.36410762342167274]\n",
      "Epoch:  776 | train loss: 0.1390\n",
      "[0.3637435157982511]\n",
      "Epoch:  777 | train loss: 0.1385\n",
      "[0.3637435157982511]\n",
      "Epoch:  778 | train loss: 0.1385\n",
      "[0.36337977228245283]\n",
      "Epoch:  779 | train loss: 0.1383\n",
      "[0.36337977228245283]\n",
      "Epoch:  780 | train loss: 0.1381\n",
      "[0.36337977228245283]\n",
      "Epoch:  781 | train loss: 0.1382\n",
      "[0.36301639251017037]\n",
      "Epoch:  782 | train loss: 0.1377\n",
      "[0.36301639251017037]\n",
      "Epoch:  783 | train loss: 0.1377\n",
      "[0.3626533761176602]\n",
      "Epoch:  784 | train loss: 0.1375\n",
      "[0.3626533761176602]\n",
      "Epoch:  785 | train loss: 0.1373\n",
      "[0.3626533761176602]\n",
      "Epoch:  786 | train loss: 0.1374\n",
      "[0.3622907227415425]\n",
      "Epoch:  787 | train loss: 0.1369\n",
      "[0.3622907227415425]\n",
      "Epoch:  788 | train loss: 0.1369\n",
      "[0.361928432018801]\n",
      "Epoch:  789 | train loss: 0.1367\n",
      "[0.361928432018801]\n",
      "Epoch:  790 | train loss: 0.1365\n",
      "[0.361928432018801]\n",
      "Epoch:  791 | train loss: 0.1366\n",
      "[0.36156650358678216]\n",
      "Epoch:  792 | train loss: 0.1361\n",
      "[0.36156650358678216]\n",
      "Epoch:  793 | train loss: 0.1362\n",
      "[0.3612049370831954]\n",
      "Epoch:  794 | train loss: 0.1360\n",
      "[0.3612049370831954]\n",
      "Epoch:  795 | train loss: 0.1357\n",
      "[0.3612049370831954]\n",
      "Epoch:  796 | train loss: 0.1359\n",
      "[0.3608437321461122]\n",
      "Epoch:  797 | train loss: 0.1353\n",
      "[0.3608437321461122]\n",
      "Epoch:  798 | train loss: 0.1354\n",
      "[0.3604828884139661]\n",
      "Epoch:  799 | train loss: 0.1352\n",
      "[0.3604828884139661]\n",
      "Epoch:  800 | train loss: 0.1350\n",
      "[0.3604828884139661]\n",
      "Epoch:  801 | train loss: 0.1351\n",
      "[0.36012240552555214]\n",
      "Epoch:  802 | train loss: 0.1346\n",
      "[0.36012240552555214]\n",
      "Epoch:  803 | train loss: 0.1346\n",
      "[0.35976228312002656]\n",
      "Epoch:  804 | train loss: 0.1345\n",
      "[0.35976228312002656]\n",
      "Epoch:  805 | train loss: 0.1342\n",
      "[0.35976228312002656]\n",
      "Epoch:  806 | train loss: 0.1344\n",
      "[0.35940252083690655]\n",
      "Epoch:  807 | train loss: 0.1338\n",
      "[0.35940252083690655]\n",
      "Epoch:  808 | train loss: 0.1339\n",
      "[0.35904311831606966]\n",
      "Epoch:  809 | train loss: 0.1337\n",
      "[0.35904311831606966]\n",
      "Epoch:  810 | train loss: 0.1335\n",
      "[0.35904311831606966]\n",
      "Epoch:  811 | train loss: 0.1336\n",
      "[0.3586840751977536]\n",
      "Epoch:  812 | train loss: 0.1331\n",
      "[0.3586840751977536]\n",
      "Epoch:  813 | train loss: 0.1332\n",
      "[0.35832539112255585]\n",
      "Epoch:  814 | train loss: 0.1330\n",
      "[0.35832539112255585]\n",
      "Epoch:  815 | train loss: 0.1328\n",
      "[0.35832539112255585]\n",
      "Epoch:  816 | train loss: 0.1329\n",
      "[0.3579670657314333]\n",
      "Epoch:  817 | train loss: 0.1324\n",
      "[0.3579670657314333]\n",
      "Epoch:  818 | train loss: 0.1325\n",
      "[0.35760909866570184]\n",
      "Epoch:  819 | train loss: 0.1323\n",
      "[0.35760909866570184]\n",
      "Epoch:  820 | train loss: 0.1321\n",
      "[0.35760909866570184]\n",
      "Epoch:  821 | train loss: 0.1322\n",
      "[0.3572514895670361]\n",
      "Epoch:  822 | train loss: 0.1317\n",
      "[0.3572514895670361]\n",
      "Epoch:  823 | train loss: 0.1318\n",
      "[0.35689423807746906]\n",
      "Epoch:  824 | train loss: 0.1316\n",
      "[0.35689423807746906]\n",
      "Epoch:  825 | train loss: 0.1314\n",
      "[0.35689423807746906]\n",
      "Epoch:  826 | train loss: 0.1315\n",
      "[0.3565373438393916]\n",
      "Epoch:  827 | train loss: 0.1310\n",
      "[0.3565373438393916]\n",
      "Epoch:  828 | train loss: 0.1311\n",
      "[0.3561808064955522]\n",
      "Epoch:  829 | train loss: 0.1309\n",
      "[0.3561808064955522]\n",
      "Epoch:  830 | train loss: 0.1307\n",
      "[0.3561808064955522]\n",
      "Epoch:  831 | train loss: 0.1309\n",
      "[0.35582462568905665]\n",
      "Epoch:  832 | train loss: 0.1304\n",
      "[0.35582462568905665]\n",
      "Epoch:  833 | train loss: 0.1304\n",
      "[0.3554688010633676]\n",
      "Epoch:  834 | train loss: 0.1303\n",
      "[0.3554688010633676]\n",
      "Epoch:  835 | train loss: 0.1301\n",
      "[0.3554688010633676]\n",
      "Epoch:  836 | train loss: 0.1302\n",
      "[0.35511333226230424]\n",
      "Epoch:  837 | train loss: 0.1297\n",
      "[0.35511333226230424]\n",
      "Epoch:  838 | train loss: 0.1298\n",
      "[0.35475821893004195]\n",
      "Epoch:  839 | train loss: 0.1296\n",
      "[0.35475821893004195]\n",
      "Epoch:  840 | train loss: 0.1294\n",
      "[0.35475821893004195]\n",
      "Epoch:  841 | train loss: 0.1295\n",
      "[0.3544034607111119]\n",
      "Epoch:  842 | train loss: 0.1290\n",
      "[0.3544034607111119]\n",
      "Epoch:  843 | train loss: 0.1291\n",
      "[0.35404905725040076]\n",
      "Epoch:  844 | train loss: 0.1290\n",
      "[0.35404905725040076]\n",
      "Epoch:  845 | train loss: 0.1288\n",
      "[0.35404905725040076]\n",
      "Epoch:  846 | train loss: 0.1289\n",
      "[0.35369500819315036]\n",
      "Epoch:  847 | train loss: 0.1284\n",
      "[0.35369500819315036]\n",
      "Epoch:  848 | train loss: 0.1285\n",
      "[0.3533413131849572]\n",
      "Epoch:  849 | train loss: 0.1283\n",
      "[0.3533413131849572]\n",
      "Epoch:  850 | train loss: 0.1281\n",
      "[0.3533413131849572]\n",
      "Epoch:  851 | train loss: 0.1283\n",
      "[0.3529879718717722]\n",
      "Epoch:  852 | train loss: 0.1278\n",
      "[0.3529879718717722]\n",
      "Epoch:  853 | train loss: 0.1279\n",
      "[0.35263498389990044]\n",
      "Epoch:  854 | train loss: 0.1277\n",
      "[0.35263498389990044]\n",
      "Epoch:  855 | train loss: 0.1275\n",
      "[0.35263498389990044]\n",
      "Epoch:  856 | train loss: 0.1277\n",
      "[0.3522823489160005]\n",
      "Epoch:  857 | train loss: 0.1272\n",
      "[0.3522823489160005]\n",
      "Epoch:  858 | train loss: 0.1272\n",
      "[0.35193006656708453]\n",
      "Epoch:  859 | train loss: 0.1271\n",
      "[0.35193006656708453]\n",
      "Epoch:  860 | train loss: 0.1269\n",
      "[0.35193006656708453]\n",
      "Epoch:  861 | train loss: 0.1270\n",
      "[0.35157813650051745]\n",
      "Epoch:  862 | train loss: 0.1266\n",
      "[0.35157813650051745]\n",
      "Epoch:  863 | train loss: 0.1266\n",
      "[0.35122655836401695]\n",
      "Epoch:  864 | train loss: 0.1265\n",
      "[0.35122655836401695]\n",
      "Epoch:  865 | train loss: 0.1263\n",
      "[0.35122655836401695]\n",
      "Epoch:  866 | train loss: 0.1264\n",
      "[0.35087533180565295]\n",
      "Epoch:  867 | train loss: 0.1260\n",
      "[0.35087533180565295]\n",
      "Epoch:  868 | train loss: 0.1261\n",
      "[0.3505244564738473]\n",
      "Epoch:  869 | train loss: 0.1259\n",
      "[0.3505244564738473]\n",
      "Epoch:  870 | train loss: 0.1257\n",
      "[0.3505244564738473]\n",
      "Epoch:  871 | train loss: 0.1259\n",
      "[0.35017393201737346]\n",
      "Epoch:  872 | train loss: 0.1254\n",
      "[0.35017393201737346]\n",
      "Epoch:  873 | train loss: 0.1255\n",
      "[0.3498237580853561]\n",
      "Epoch:  874 | train loss: 0.1253\n",
      "[0.3498237580853561]\n",
      "Epoch:  875 | train loss: 0.1251\n",
      "[0.3498237580853561]\n",
      "Epoch:  876 | train loss: 0.1253\n",
      "[0.34947393432727075]\n",
      "Epoch:  877 | train loss: 0.1248\n",
      "[0.34947393432727075]\n",
      "Epoch:  878 | train loss: 0.1249\n",
      "[0.3491244603929435]\n",
      "Epoch:  879 | train loss: 0.1248\n",
      "[0.3491244603929435]\n",
      "Epoch:  880 | train loss: 0.1246\n",
      "[0.3491244603929435]\n",
      "Epoch:  881 | train loss: 0.1247\n",
      "[0.34877533593255056]\n",
      "Epoch:  882 | train loss: 0.1243\n",
      "[0.34877533593255056]\n",
      "Epoch:  883 | train loss: 0.1244\n",
      "[0.34842656059661803]\n",
      "Epoch:  884 | train loss: 0.1242\n",
      "[0.34842656059661803]\n",
      "Epoch:  885 | train loss: 0.1240\n",
      "[0.34842656059661803]\n",
      "Epoch:  886 | train loss: 0.1242\n",
      "[0.3480781340360214]\n",
      "Epoch:  887 | train loss: 0.1237\n",
      "[0.3480781340360214]\n",
      "Epoch:  888 | train loss: 0.1238\n",
      "[0.34773005590198536]\n",
      "Epoch:  889 | train loss: 0.1237\n",
      "[0.34773005590198536]\n",
      "Epoch:  890 | train loss: 0.1235\n",
      "[0.34773005590198536]\n",
      "Epoch:  891 | train loss: 0.1236\n",
      "[0.34738232584608336]\n",
      "Epoch:  892 | train loss: 0.1232\n",
      "[0.34738232584608336]\n",
      "Epoch:  893 | train loss: 0.1233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3470349435202373]\n",
      "Epoch:  894 | train loss: 0.1231\n",
      "[0.3470349435202373]\n",
      "Epoch:  895 | train loss: 0.1230\n",
      "[0.3470349435202373]\n",
      "Epoch:  896 | train loss: 0.1231\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir)\n",
    "print(outputs)\n",
    "running_loss = 0\n",
    "import random\n",
    "counter = 0\n",
    "for epoch in range(1000000):\n",
    "    hidden_state = rnn.init_hidden() \n",
    "    x = random.randint(0,20)\n",
    "    if x == 0:\n",
    "        dailyInputs = hr[:int(splitInputs[x])]            \n",
    "    else:           \n",
    "        dailyInputs = hr[int(sum(splitInputs[:x])) :int(sum(splitInputs[:x])) + int(splitInputs[x])]\n",
    "    for y in range(int(splitInputs[x])):            \n",
    "        output, hidden_state = rnn(dailyInputs[y].view(1,3), hidden_state)                               # rnn output\n",
    "       \n",
    "\n",
    "    hidden = hidden_state\n",
    "    loss = loss_fn(output, outputs[x].view(1, 1))                   #  loss\n",
    "    optimizer.zero_grad()                           # clear gradients for this training step\n",
    "    loss.backward()                                 # backpropagation, compute gradients\n",
    "    optimizer.step() \n",
    "    scheduler.step()\n",
    "        # apply gradients\n",
    "    running_loss += loss.item()\n",
    "    writer.add_scalar('running loss', running_loss, x + epoch * (20) )\n",
    "    running_loss = 0\n",
    "\n",
    "            \n",
    "            \n",
    "    print(scheduler.get_last_lr())\n",
    "    \n",
    "    print('Epoch: ', epoch, '| train loss: %.4f' % loss.data)\n",
    "\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea9292f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9316), started 1:11:20 ago. (Use '!kill 9316' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-402509de2bb594c0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-402509de2bb594c0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2097781",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(splitInputs)):\n",
    "    outputlist = []\n",
    "\n",
    "    output = 0\n",
    "    \n",
    "    if x == 0:\n",
    "        for y in range(len(hr[:splitInputs[x]])):\n",
    "            outputlist.append(rnn(hr[y]).detach().numpy()[0])\n",
    "    else:\n",
    "        for y in range(len(hr[np.sum(splitInputs[x-1]):np.sum(splitInputs[x-1] + splitInputs[x])])):\n",
    "            outputlist.append(rnn(hr[y]).detach().numpy()[0]   )                \n",
    "\n",
    "    outputlist = np.reshape(outputlist, (-1,1))\n",
    "    outputlist = outputScaler.inverse_transform(outputlist)\n",
    "    output = torch.FloatTensor([np.sum(outputlist)])\n",
    "    \n",
    "    calories = torch.FloatTensor(calories)\n",
    "    loss = loss_fn(output.view(1,1), calories[x].view(1,1))\n",
    "\n",
    "    print(output, \" : \", calories[x], 'train loss: %.4f' % loss.data)\n",
    "    \n",
    "    \n",
    "outputList = np.reshape(outputList, (-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputList = []\n",
    "for x in range(36):\n",
    "    hidden_state = rnn.init_hidden()\n",
    "\n",
    "    if x == 0:\n",
    "        dailyInputs = hr[:int(splitInputs[x])]            \n",
    "    else:           \n",
    "        dailyInputs = hr[int(sum(splitInputs[:x])) :int(sum(splitInputs[:x])) + int(splitInputs[x])]\n",
    "    \n",
    "    for y in range(int(splitInputs[x])):     \n",
    "        output, hidden_state = rnn(dailyInputs[y].view(1,2), hidden_state)\n",
    "   \n",
    "    outputList.append(output.detach().numpy())\n",
    "    print(output)\n",
    "    print(outputs[x].view(1,1))\n",
    "    loss = loss_fn(output, outputs[x].view(1, 1))  \n",
    "    print(output[0][0], \" : \", outputs[x][0], 'train loss: %.4f' % loss.data)\n",
    "    \n",
    "outputList = np.reshape(outputList, (-1,1))\n",
    "print(outputScaler.inverse_transform(outputList))\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
