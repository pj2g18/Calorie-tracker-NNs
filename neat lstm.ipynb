{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weight-2021-03-05.json']\n",
      "[15.30000019 14.6        13.89999962 13.19999981 13.19999981 14.60000038\n",
      " 14.60000038 14.60000038 13.89999962 13.89999962 13.89999962 14.60000038\n",
      " 14.60000038 13.19999981 13.19999981]\n",
      "[114.6 114.6 112.  112.8 112.8 112.  112.2 110.  110.8 109.8 109.8 110.4\n",
      " 107.6 109.8 108.7]\n"
     ]
    }
   ],
   "source": [
    "#things to fill out\n",
    "rootdir = \"maddy driscoll/MyFitbitData5/Mad/personal & Account\"\n",
    "regex = re.compile('weight-2021-03-05')\n",
    "startDate = '03/13/21' \n",
    "endDate = '03/28/21'\n",
    "\n",
    "#get time period\n",
    "startd = datetime.datetime.strptime(startDate, \"%m/%d/%y\")\n",
    "endd = datetime.datetime.strptime(endDate, \"%m/%d/%y\")\n",
    "date_array = (startd + datetime.timedelta(days=x) for x in range(0, (endd-startd).days))\n",
    "dates = []\n",
    "for date_object in date_array:\n",
    "    dates = np.append(dates, date_object.strftime(\"%m/%d/%y\"))\n",
    "\n",
    "    \n",
    "#get weight files\n",
    "filenames = os.listdir(rootdir)\n",
    "weights= []\n",
    "for root, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if regex.match(file):\n",
    "            weights = np.append(weights, file)\n",
    "print(weights)            \n",
    "\n",
    "\n",
    "#get weight data\n",
    "roughdata = []\n",
    "for x in range(np.size(weights)):\n",
    "    filename = rootdir + \"/\" + weights[x]    \n",
    "    data = []\n",
    "    weight = []\n",
    "    fat = []\n",
    "    with open(filename) as json_file:\n",
    "        rawdata = json.load(json_file)\n",
    "        for n in range(np.size(rawdata)):\n",
    "            date = rawdata[n][\"date\"]\n",
    "            if date in dates:\n",
    "                roughdata.append(rawdata[n])  \n",
    "                \n",
    "        \n",
    "        for x in range(np.size(roughdata)):\n",
    "            \n",
    "            del roughdata[x][\"logId\"]\n",
    "            del roughdata[x][\"time\"]\n",
    "            del roughdata[x][\"source\"]\n",
    "            del roughdata[x][\"bmi\"]\n",
    "            weight = np.append(weight, roughdata[x][\"weight\"])\n",
    "            fat = np.append(fat, roughdata[x][\"fat\"])\n",
    "\n",
    "        data = np.append(data, roughdata)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print(fat)\n",
    "\n",
    "print(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1052.]\n",
      " [ 932.]\n",
      " [ 601.]\n",
      " [ 810.]\n",
      " [ 775.]\n",
      " [ 757.]\n",
      " [1021.]\n",
      " [2191.]\n",
      " [ 658.]\n",
      " [ 744.]\n",
      " [1097.]\n",
      " [ 632.]\n",
      " [ 857.]\n",
      " [1611.]\n",
      " [1261.]]\n"
     ]
    }
   ],
   "source": [
    "#things to fill out\n",
    "rootdir = \"maddy driscoll/MyFitbitData5/Mad/Nutrition\"\n",
    "regex = re.compile('food_logs-0')\n",
    "startD = \"2021-03-12\"\n",
    "endD = \"2021-03-27\"\n",
    "\n",
    "\n",
    "#get calorie data\n",
    "filenames = os.listdir(rootdir)\n",
    "foodfiles = []\n",
    "for root, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if regex.match(file):\n",
    "            foodfiles = np.append(foodfiles, file)\n",
    "\n",
    "start = False\n",
    "#get food entries\n",
    "for x in range(np.size(foodfiles)):\n",
    "    filename = rootdir + \"/\" + foodfiles[x]\n",
    "    data = []\n",
    "    count = 0\n",
    "    with open(filename) as json_file:\n",
    "        roughdata = json.load(json_file)\n",
    "        for y in range(np.size(roughdata)):\n",
    "            if roughdata[y-count][\"logDate\"] == startD:\n",
    "                start = False\n",
    "            if roughdata[y-count][\"logDate\"] == endD:\n",
    "                start = True\n",
    "            if start == True:\n",
    "                cal = roughdata[y-count][\"loggedFood\"][\"calories\"]                \n",
    "                del roughdata[y-count][\"logId\"]\n",
    "                del roughdata[y-count][\"loggedFood\"]\n",
    "                del roughdata[y-count][\"favorite\"]\n",
    "                if \"nutritionalValues\" in roughdata[y-count].keys():                \n",
    "                    del roughdata[y-count][\"nutritionalValues\"]                \n",
    "                roughdata[y-count][\"calories\"] = cal\n",
    "            else:\n",
    "                del roughdata[y-count]\n",
    "                count +=1\n",
    "        \n",
    "        data = np.append(data, roughdata)\n",
    "        \n",
    "        \n",
    "#get calories for each day\n",
    "calories = data[0][\"calories\"]\n",
    "totCalories = np.empty((0,1))\n",
    "dates = np.empty((0,1))\n",
    "for x in range(len(data)-1):\n",
    "    if data[x+1][\"logDate\"] == data[x][\"logDate\"]:\n",
    "        calories += data[x+1][\"calories\"]\n",
    "        if x == len(data)-2:\n",
    "            totCalories = np.vstack((totCalories, calories))\n",
    "    else:\n",
    "        dates = np.vstack((dates, data[x][\"logDate\"]))\n",
    "        totCalories = np.vstack((totCalories, calories))\n",
    "        calories = data[x+1][\"calories\"]\n",
    "    \n",
    "\n",
    "totCalories = np.flip(totCalories)\n",
    "print(totCalories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-142.30229677 -615.3131631    91.65866162   -0.          488.58337522\n",
      "   58.17978814 -273.61167113   95.13531859 -121.76220858   -0.\n",
      "  360.84063081 -348.23303598  367.29365004 -131.07102722    0.        ]\n",
      "[1194.30229677 1547.3131631   509.34133838  810.          286.41662478\n",
      "  698.82021186 1294.61167113 2095.86468141  779.76220858  744.\n",
      "  736.15936919  980.23303598  489.70634996 1742.07102722 1261.        ]\n"
     ]
    }
   ],
   "source": [
    "#calculate calories burned more/less than consumed based on weight change\n",
    "def caloriesBurned(weight, fat):\n",
    "    fatMass = []\n",
    "    leanMass = []\n",
    "    for x in range(len(weight)):\n",
    "        fatMass = np.append(fatMass, weight[x]*fat[x]/100)\n",
    "        leanMass = np.append(leanMass, weight[x] - fatMass[x])\n",
    "    calories = [] \n",
    "    for x in range(len(weight) - 1):   \n",
    "        calories = np.append(calories, -(fatMass[x]-fatMass[x+1])*442.39)\n",
    "        if leanMass[x] >= leanMass[x+1]:\n",
    "            calories[x] -= (leanMass[x] - leanMass[x+1])*70\n",
    "        else:\n",
    "            calories[x] -= (leanMass[x]-leanMass[x+1])*265\n",
    "    calories = np.append(calories, [0])\n",
    "\n",
    "    return calories\n",
    "        \n",
    "#calculate total calories burned in a day\n",
    "calories = caloriesBurned(weight, fat)\n",
    "print(calories)\n",
    "for x in range(len(calories)):\n",
    "    calories[x] = totCalories[x] - calories[x]\n",
    "\n",
    "#calories = np.append(calories, [2000,2000, 2000])\n",
    "print(calories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000e+01 4.0000e+01 1.0000e+00]\n",
      " [5.0000e+00 4.1000e+01 1.0000e+00]\n",
      " [1.5000e+01 4.0000e+01 1.0000e+00]\n",
      " ...\n",
      " [5.0000e+00 5.1000e+01 1.5000e+01]\n",
      " [5.0000e+00 5.0000e+01 1.5000e+01]\n",
      " [8.6389e+04 5.1000e+01 1.5000e+01]]\n",
      "tensor([[[-0.9944, -0.9096, -1.0000],\n",
      "         [-0.9975, -0.8983, -1.0000],\n",
      "         [-0.9913, -0.9096, -1.0000],\n",
      "         ...,\n",
      "         [-0.9913, -0.8870, -1.0000],\n",
      "         [-0.9944, -0.9209, -1.0000],\n",
      "         [-0.9975, -0.8983, -1.0000]],\n",
      "\n",
      "        [[-0.9975, -0.8870, -1.0000],\n",
      "         [-0.9975, -0.8870, -1.0000],\n",
      "         [-0.9913, -0.8757, -1.0000],\n",
      "         ...,\n",
      "         [-0.9975, -0.1186, -1.0000],\n",
      "         [-0.9944, -0.1299, -1.0000],\n",
      "         [-0.9975, -0.1638, -1.0000]],\n",
      "\n",
      "        [[-0.9975, -0.1525, -1.0000],\n",
      "         [-0.9944, -0.1412, -1.0000],\n",
      "         [-0.9975, -0.1751, -1.0000],\n",
      "         ...,\n",
      "         [-0.9913, -0.7288, -1.0000],\n",
      "         [-0.9975, -0.7288, -1.0000],\n",
      "         [-0.9969, -0.7062, -1.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0000, -0.4011,  1.0000],\n",
      "         [-0.9988, -0.4124,  1.0000],\n",
      "         [-0.9994, -0.4124,  1.0000],\n",
      "         ...,\n",
      "         [-0.9944, -0.8757,  1.0000],\n",
      "         [-0.9975, -0.8644,  1.0000],\n",
      "         [-0.9944, -0.8531,  1.0000]],\n",
      "\n",
      "        [[-0.9975, -0.8418,  1.0000],\n",
      "         [-0.9913, -0.8305,  1.0000],\n",
      "         [-0.9913, -0.8531,  1.0000],\n",
      "         ...,\n",
      "         [-0.9975, -0.6723,  1.0000],\n",
      "         [-0.9975, -0.6949,  1.0000],\n",
      "         [-0.9975, -0.6949,  1.0000]],\n",
      "\n",
      "        [[-0.9975, -0.6836,  1.0000],\n",
      "         [-0.9913, -0.6836,  1.0000],\n",
      "         [-0.9975, -0.6949,  1.0000],\n",
      "         ...,\n",
      "         [-0.9975, -0.7966,  1.0000],\n",
      "         [-0.9975, -0.7853,  1.0000],\n",
      "         [-0.9975, -0.7966,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "#things to fill in\n",
    "rootdir = \"maddy driscoll/MyFitbitData5/Mad/Physical Activity\"\n",
    "regex = re.compile('heart_rate-2021-03')\n",
    "listSplitValue = 2677\n",
    "#get heart rates\n",
    "\n",
    "filenames = os.listdir(rootdir)\n",
    "heartRates = []\n",
    "for root, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if regex.match(file):\n",
    "            heartRates = np.append(heartRates, file)\n",
    "\n",
    "\n",
    "#read files for hr data\n",
    "inputs = np.empty((0,3))\n",
    "dateTime = []\n",
    "start = False\n",
    "minInputs = np.empty((0,3))\n",
    "hrDates = np.empty((0,1))\n",
    "count = 1\n",
    "for x in range(np.size(heartRates)):\n",
    "    filename = rootdir + \"/\" + heartRates[x]\n",
    "    data = []\n",
    "    \n",
    "    with open(filename) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        inputs = np.empty((len(data), 3))\n",
    "        for y in range(len(data)):\n",
    "            temp = data[y][\"dateTime\"][9:] \n",
    "            \n",
    "            \n",
    "            if data[y][\"dateTime\"][:8] == startDate:\n",
    "                start = True\n",
    "            \n",
    "                \n",
    "            if data[y][\"dateTime\"][:8] == endDate:\n",
    "                start = False\n",
    "            if start == True:\n",
    "                inputs[y][0] = int(temp[:2])*3600  + int(temp[3:5])*60 + int(temp[6:9])\n",
    "                inputs[y][1] = int(data[y][\"value\"][\"bpm\"])\n",
    "                inputs[y][2] = count\n",
    "            \n",
    "       # hrDates = np.vstack((hrDates, data[10][\"dateTime\"][:8]))\n",
    "        if start == True:\n",
    "            count += 1\n",
    "            \n",
    "            minInputs = np.vstack((minInputs, inputs))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "for x in range(len(minInputs) - 1):\n",
    "    if minInputs[x][0] <= minInputs [x+1][0]:\n",
    "        minInputs[x][0] = minInputs[x+1][0] - minInputs[x][0]\n",
    "    else:\n",
    "        minInputs[x][0] = 86400 - minInputs[x][0] + minInputs[x+1][0]\n",
    "        \n",
    "        \n",
    "print(minInputs)    \n",
    "\n",
    "#normalize, scale and format\n",
    "minInputs = np.delete(minInputs, len(minInputs)-1, 0)\n",
    "train_data_normalized2 = scaler.fit_transform(np.transpose(minInputs)[1] .reshape(-1,1))\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "train_data_normalized1 = scaler.fit_transform(np.transpose(minInputs)[0] .reshape(-1,1))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "train_data_normalized3 = scaler.fit_transform(np.transpose(minInputs)[2] .reshape(-1,1))\n",
    "\n",
    "train_data_normalized = np.hstack((train_data_normalized1, train_data_normalized2))\n",
    "\n",
    "train_data_normalized = np.hstack((train_data_normalized, train_data_normalized3))\n",
    "train_data_normalized =torch.FloatTensor(train_data_normalized)\n",
    "inputs = train_data_normalized.view(-1, listSplitValue, train_data_normalized.shape[1])\n",
    "print(inputs)\n",
    "numOfInputs = np.size(np.transpose(minInputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woo\n",
      "yay\n",
      "torch.Size([168651, 1])\n",
      "torch.Size([63, 2677, 1])\n",
      "[0.13822943 0.06911472 0.20734415 ... 0.07297454 0.07297454 0.07297454]\n",
      "tensor([[-0.9931],\n",
      "        [-0.9966],\n",
      "        [-0.9896],\n",
      "        ...,\n",
      "        [-0.9964],\n",
      "        [-0.9964],\n",
      "        [-0.9964]])\n",
      "tensor([[[-0.9931],\n",
      "         [-0.9966],\n",
      "         [-0.9896],\n",
      "         ...,\n",
      "         [-0.9896],\n",
      "         [-0.9931],\n",
      "         [-0.9966]],\n",
      "\n",
      "        [[-0.9966],\n",
      "         [-0.9966],\n",
      "         [-0.9896],\n",
      "         ...,\n",
      "         [-0.9966],\n",
      "         [-0.9931],\n",
      "         [-0.9966]],\n",
      "\n",
      "        [[-0.9966],\n",
      "         [-0.9931],\n",
      "         [-0.9966],\n",
      "         ...,\n",
      "         [-0.9896],\n",
      "         [-0.9966],\n",
      "         [-0.9959]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9994],\n",
      "         [-0.9979],\n",
      "         [-0.9987],\n",
      "         ...,\n",
      "         [-0.9927],\n",
      "         [-0.9964],\n",
      "         [-0.9927]],\n",
      "\n",
      "        [[-0.9964],\n",
      "         [-0.9890],\n",
      "         [-0.9890],\n",
      "         ...,\n",
      "         [-0.9964],\n",
      "         [-0.9964],\n",
      "         [-0.9964]],\n",
      "\n",
      "        [[-0.9964],\n",
      "         [-0.9890],\n",
      "         [-0.9964],\n",
      "         ...,\n",
      "         [-0.9964],\n",
      "         [-0.9964],\n",
      "         [-0.9964]]])\n"
     ]
    }
   ],
   "source": [
    "#calculate expected outputs\n",
    "y_actual = []\n",
    "day = 0\n",
    "print(\"woo\")\n",
    "for x in range(numOfInputs):\n",
    "    interval = minInputs[x][0]\n",
    "  \n",
    "    day = minInputs[x][2]\n",
    "    \n",
    "    intervalCalorie = calories[int(day-1)]/86400 * interval        \n",
    "    y_actual = np.append(y_actual, intervalCalorie)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "print(\"yay\")\n",
    "normalized_y = torch.FloatTensor(scaler.fit_transform(y_actual .reshape(-1,1)))\n",
    "print(np.shape(normalized_y))\n",
    "\n",
    "outputs = normalized_y.view(-1, listSplitValue, normalized_y.shape[1])\n",
    "print(np.shape(outputs))\n",
    "print(y_actual)\n",
    "print(normalized_y)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\.conda\\envs\\ailab\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "'''class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 3, hidden_size = 10, output_size = 1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, 1)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)        \n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq),1,-1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out)\n",
    "        return predictions\n",
    "    \n",
    "n_hidden = 5\n",
    "lstm = RNN()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "print(lstm)'''\n",
    "\n",
    "\n",
    "#backpropogate and feedbackward part of rnn\n",
    "class RNN(nn.Module):\n",
    " \n",
    "    def __init__(self, input_dim=3, hidden_dim=20, batch_size=listSplitValue, output_dim=1,\n",
    "                    num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    " \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    " \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    " \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    " \n",
    "    def forward(self, input_seq):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input_seq.view(len(input_seq),1,-1))     \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred\n",
    "lstm = RNN()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimiser = torch.optim.Adam(lstm.parameters(), lr=0.01)\n",
    "print(\"Yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size = 1, hidden_size = 10, output_size = 1):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRNN(\n",
      "  (in2hidden): Linear(in_features=11, out_features=10, bias=True)\n",
      "  (in2output): Linear(in_features=11, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\.conda\\envs\\ailab\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "rnn = MyRNN()\n",
    "print(rnn)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)   # optimize all cnn parameters\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x in range(BATCH_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 2677, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\.conda\\envs\\ailab\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([2677, 1])) that is different to the input size (torch.Size([2677, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6968/1278995545.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msingle_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2677\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ailab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6968/3943621403.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_seq)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# shape of self.hidden: (a, b), where a and b both\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# have shape (num_layers, batch_size, hidden_dim).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Only take the output from the final timetep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ailab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ailab\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 692\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "print(np.shape(inputs))\n",
    "for i in range(epochs):\n",
    "    for x in range(40): \n",
    "        lstm.zero_grad()\n",
    "\n",
    "        y_pred = lstm(inputs[x])\n",
    "        \n",
    "        single_loss = loss_fn(y_pred.float(), outputs[x].view(2677, 1, 1).float())\n",
    "        loss = loss_fn(y_pred, outputs[x])\n",
    "      \n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "print(\"WOOOOO\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = MyRNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 45\n",
    "y_pred = lstm(inputs[num])\n",
    "single_loss = loss_function(y_pred.float(), outputs[0].float())\n",
    "yo = y_pred.detach().numpy()\n",
    "\n",
    "woo =scaler.inverse_transform(np.transpose(np.transpose(yo)[0]))\n",
    "print(np.sum(woo))\n",
    "#print(woo)\n",
    "outs = scaler.inverse_transform(outputs[num])\n",
    "print(np.sum(outs))\n",
    "print(np.hstack((woo,outs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 100       # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = listSplitValue\n",
    "INPUT_SIZE = 3      # rnn input size / image width\n",
    "LR = 0.01            # learning rate\n",
    "DOWNLOAD_MNIST = True   # set to True if haven't download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Loader for easy mini-batch return in training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=inputs,\n",
    "                                           batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=20,         # rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x.view(len(x),1,-1))   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(3, 20, batch_first=True)\n",
      "  (out): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2677, 1, 1])\n",
      "Epoch:  0 | train loss: 1391.4951\n",
      "Epoch:  1 | train loss: 1195.2126\n",
      "Epoch:  2 | train loss: 1003.1124\n",
      "Epoch:  3 | train loss: 814.1017\n",
      "Epoch:  4 | train loss: 631.0071\n",
      "Epoch:  5 | train loss: 458.6605\n",
      "Epoch:  6 | train loss: 303.3830\n",
      "Epoch:  7 | train loss: 172.6074\n",
      "Epoch:  8 | train loss: 74.2973\n",
      "Epoch:  9 | train loss: 15.7048\n",
      "Epoch:  10 | train loss: 0.8494\n",
      "Epoch:  11 | train loss: 26.0911\n",
      "Epoch:  12 | train loss: 75.5567\n",
      "Epoch:  13 | train loss: 124.6997\n",
      "Epoch:  14 | train loss: 154.2218\n",
      "Epoch:  15 | train loss: 158.0511\n",
      "Epoch:  16 | train loss: 140.3262\n",
      "Epoch:  17 | train loss: 109.6635\n",
      "Epoch:  18 | train loss: 75.0507\n",
      "Epoch:  19 | train loss: 43.6513\n",
      "Epoch:  20 | train loss: 19.9369\n",
      "Epoch:  21 | train loss: 5.6695\n",
      "Epoch:  22 | train loss: 0.4022\n",
      "Epoch:  23 | train loss: 2.2349\n",
      "Epoch:  24 | train loss: 8.5979\n",
      "Epoch:  25 | train loss: 16.8979\n",
      "Epoch:  26 | train loss: 24.9454\n",
      "Epoch:  27 | train loss: 31.1691\n",
      "Epoch:  28 | train loss: 34.6644\n",
      "Epoch:  29 | train loss: 35.1386\n",
      "Epoch:  30 | train loss: 32.8026\n",
      "Epoch:  31 | train loss: 28.2402\n",
      "Epoch:  32 | train loss: 22.2729\n",
      "Epoch:  33 | train loss: 15.8243\n",
      "Epoch:  34 | train loss: 9.7902\n",
      "Epoch:  35 | train loss: 4.9182\n",
      "Epoch:  36 | train loss: 1.7061\n",
      "Epoch:  37 | train loss: 0.3350\n",
      "Epoch:  38 | train loss: 0.6512\n",
      "Epoch:  39 | train loss: 2.2115\n",
      "Epoch:  40 | train loss: 4.3876\n",
      "Epoch:  41 | train loss: 6.5124\n",
      "Epoch:  42 | train loss: 8.0293\n",
      "Epoch:  43 | train loss: 8.6060\n",
      "Epoch:  44 | train loss: 8.1808\n",
      "Epoch:  45 | train loss: 6.9354\n",
      "Epoch:  46 | train loss: 5.2117\n",
      "Epoch:  47 | train loss: 3.4035\n",
      "Epoch:  48 | train loss: 1.8558\n",
      "Epoch:  49 | train loss: 0.7963\n",
      "Epoch:  50 | train loss: 0.3080\n",
      "Epoch:  51 | train loss: 0.3400\n",
      "Epoch:  52 | train loss: 0.7453\n",
      "Epoch:  53 | train loss: 1.3305\n",
      "Epoch:  54 | train loss: 1.9053\n",
      "Epoch:  55 | train loss: 2.3197\n",
      "Epoch:  56 | train loss: 2.4869\n",
      "Epoch:  57 | train loss: 2.3894\n",
      "Epoch:  58 | train loss: 2.0705\n",
      "Epoch:  59 | train loss: 1.6151\n",
      "Epoch:  60 | train loss: 1.1261\n",
      "Epoch:  61 | train loss: 0.6995\n",
      "Epoch:  62 | train loss: 0.4047\n",
      "Epoch:  63 | train loss: 0.2724\n",
      "Epoch:  64 | train loss: 0.2927\n",
      "Epoch:  65 | train loss: 0.4226\n",
      "Epoch:  66 | train loss: 0.6009\n",
      "Epoch:  67 | train loss: 0.7651\n",
      "Epoch:  68 | train loss: 0.8675\n",
      "Epoch:  69 | train loss: 0.8842\n",
      "Epoch:  70 | train loss: 0.8179\n",
      "Epoch:  71 | train loss: 0.6927\n",
      "Epoch:  72 | train loss: 0.5442\n",
      "Epoch:  73 | train loss: 0.4081\n",
      "Epoch:  74 | train loss: 0.3115\n",
      "Epoch:  75 | train loss: 0.2667\n",
      "Epoch:  76 | train loss: 0.2713\n",
      "Epoch:  77 | train loss: 0.3110\n",
      "Epoch:  78 | train loss: 0.3656\n",
      "Epoch:  79 | train loss: 0.4150\n",
      "Epoch:  80 | train loss: 0.4442\n",
      "Epoch:  81 | train loss: 0.4465\n",
      "Epoch:  82 | train loss: 0.4236\n",
      "Epoch:  83 | train loss: 0.3837\n",
      "Epoch:  84 | train loss: 0.3381\n",
      "Epoch:  85 | train loss: 0.2983\n",
      "Epoch:  86 | train loss: 0.2721\n",
      "Epoch:  87 | train loss: 0.2628\n",
      "Epoch:  88 | train loss: 0.2685\n",
      "Epoch:  89 | train loss: 0.2836\n",
      "Epoch:  90 | train loss: 0.3012\n",
      "Epoch:  91 | train loss: 0.3147\n",
      "Epoch:  92 | train loss: 0.3200\n",
      "Epoch:  93 | train loss: 0.3163\n",
      "Epoch:  94 | train loss: 0.3053\n",
      "Epoch:  95 | train loss: 0.2908\n",
      "Epoch:  96 | train loss: 0.2770\n",
      "Epoch:  97 | train loss: 0.2670\n",
      "Epoch:  98 | train loss: 0.2626\n",
      "Epoch:  99 | train loss: 0.2635\n",
      "Epoch:  100 | train loss: 0.2681\n",
      "Epoch:  101 | train loss: 0.2738\n",
      "Epoch:  102 | train loss: 0.2785\n",
      "Epoch:  103 | train loss: 0.2806\n",
      "Epoch:  104 | train loss: 0.2796\n",
      "Epoch:  105 | train loss: 0.2761\n",
      "Epoch:  106 | train loss: 0.2713\n",
      "Epoch:  107 | train loss: 0.2667\n",
      "Epoch:  108 | train loss: 0.2635\n",
      "Epoch:  109 | train loss: 0.2621\n",
      "Epoch:  110 | train loss: 0.2625\n",
      "Epoch:  111 | train loss: 0.2641\n",
      "Epoch:  112 | train loss: 0.2660\n",
      "Epoch:  113 | train loss: 0.2675\n",
      "Epoch:  114 | train loss: 0.2680\n",
      "Epoch:  115 | train loss: 0.2674\n",
      "Epoch:  116 | train loss: 0.2660\n",
      "Epoch:  117 | train loss: 0.2643\n",
      "Epoch:  118 | train loss: 0.2629\n",
      "Epoch:  119 | train loss: 0.2619\n",
      "Epoch:  120 | train loss: 0.2617\n",
      "Epoch:  121 | train loss: 0.2620\n",
      "Epoch:  122 | train loss: 0.2626\n",
      "Epoch:  123 | train loss: 0.2632\n",
      "Epoch:  124 | train loss: 0.2636\n",
      "Epoch:  125 | train loss: 0.2635\n",
      "Epoch:  126 | train loss: 0.2631\n",
      "Epoch:  127 | train loss: 0.2626\n",
      "Epoch:  128 | train loss: 0.2620\n",
      "Epoch:  129 | train loss: 0.2615\n",
      "Epoch:  130 | train loss: 0.2613\n",
      "Epoch:  131 | train loss: 0.2614\n",
      "Epoch:  132 | train loss: 0.2615\n",
      "Epoch:  133 | train loss: 0.2617\n",
      "Epoch:  134 | train loss: 0.2619\n",
      "Epoch:  135 | train loss: 0.2619\n",
      "Epoch:  136 | train loss: 0.2617\n",
      "Epoch:  137 | train loss: 0.2615\n",
      "Epoch:  138 | train loss: 0.2613\n",
      "Epoch:  139 | train loss: 0.2611\n",
      "Epoch:  140 | train loss: 0.2610\n",
      "Epoch:  141 | train loss: 0.2610\n",
      "Epoch:  142 | train loss: 0.2610\n",
      "Epoch:  143 | train loss: 0.2610\n",
      "Epoch:  144 | train loss: 0.2611\n",
      "Epoch:  145 | train loss: 0.2610\n",
      "Epoch:  146 | train loss: 0.2610\n",
      "Epoch:  147 | train loss: 0.2609\n",
      "Epoch:  148 | train loss: 0.2608\n",
      "Epoch:  149 | train loss: 0.2607\n",
      "Epoch:  150 | train loss: 0.2606\n",
      "Epoch:  151 | train loss: 0.2606\n",
      "Epoch:  152 | train loss: 0.2606\n",
      "Epoch:  153 | train loss: 0.2605\n",
      "Epoch:  154 | train loss: 0.2605\n",
      "Epoch:  155 | train loss: 0.2605\n",
      "Epoch:  156 | train loss: 0.2604\n",
      "Epoch:  157 | train loss: 0.2604\n",
      "Epoch:  158 | train loss: 0.2603\n",
      "Epoch:  159 | train loss: 0.2603\n",
      "Epoch:  160 | train loss: 0.2602\n",
      "Epoch:  161 | train loss: 0.2602\n",
      "Epoch:  162 | train loss: 0.2602\n",
      "Epoch:  163 | train loss: 0.2601\n",
      "Epoch:  164 | train loss: 0.2601\n",
      "Epoch:  165 | train loss: 0.2601\n",
      "Epoch:  166 | train loss: 0.2600\n",
      "Epoch:  167 | train loss: 0.2600\n",
      "Epoch:  168 | train loss: 0.2599\n",
      "Epoch:  169 | train loss: 0.2599\n",
      "Epoch:  170 | train loss: 0.2598\n",
      "Epoch:  171 | train loss: 0.2598\n",
      "Epoch:  172 | train loss: 0.2597\n",
      "Epoch:  173 | train loss: 0.2597\n",
      "Epoch:  174 | train loss: 0.2597\n",
      "Epoch:  175 | train loss: 0.2596\n",
      "Epoch:  176 | train loss: 0.2596\n",
      "Epoch:  177 | train loss: 0.2595\n",
      "Epoch:  178 | train loss: 0.2595\n",
      "Epoch:  179 | train loss: 0.2595\n",
      "Epoch:  180 | train loss: 0.2594\n",
      "Epoch:  181 | train loss: 0.2594\n",
      "Epoch:  182 | train loss: 0.2593\n",
      "Epoch:  183 | train loss: 0.2593\n",
      "Epoch:  184 | train loss: 0.2593\n",
      "Epoch:  185 | train loss: 0.2592\n",
      "Epoch:  186 | train loss: 0.2592\n",
      "Epoch:  187 | train loss: 0.2591\n",
      "Epoch:  188 | train loss: 0.2591\n",
      "Epoch:  189 | train loss: 0.2590\n",
      "Epoch:  190 | train loss: 0.2590\n",
      "Epoch:  191 | train loss: 0.2590\n",
      "Epoch:  192 | train loss: 0.2589\n",
      "Epoch:  193 | train loss: 0.2589\n",
      "Epoch:  194 | train loss: 0.2588\n",
      "Epoch:  195 | train loss: 0.2588\n",
      "Epoch:  196 | train loss: 0.2587\n",
      "Epoch:  197 | train loss: 0.2587\n",
      "Epoch:  198 | train loss: 0.2587\n",
      "Epoch:  199 | train loss: 0.2586\n",
      "Epoch:  200 | train loss: 0.2586\n",
      "Epoch:  201 | train loss: 0.2585\n",
      "Epoch:  202 | train loss: 0.2585\n",
      "Epoch:  203 | train loss: 0.2584\n",
      "Epoch:  204 | train loss: 0.2584\n",
      "Epoch:  205 | train loss: 0.2584\n",
      "Epoch:  206 | train loss: 0.2583\n",
      "Epoch:  207 | train loss: 0.2583\n",
      "Epoch:  208 | train loss: 0.2582\n",
      "Epoch:  209 | train loss: 0.2582\n",
      "Epoch:  210 | train loss: 0.2581\n",
      "Epoch:  211 | train loss: 0.2581\n",
      "Epoch:  212 | train loss: 0.2581\n",
      "Epoch:  213 | train loss: 0.2580\n",
      "Epoch:  214 | train loss: 0.2580\n",
      "Epoch:  215 | train loss: 0.2579\n",
      "Epoch:  216 | train loss: 0.2579\n",
      "Epoch:  217 | train loss: 0.2578\n",
      "Epoch:  218 | train loss: 0.2578\n",
      "Epoch:  219 | train loss: 0.2577\n",
      "Epoch:  220 | train loss: 0.2577\n",
      "Epoch:  221 | train loss: 0.2577\n",
      "Epoch:  222 | train loss: 0.2576\n",
      "Epoch:  223 | train loss: 0.2576\n",
      "Epoch:  224 | train loss: 0.2575\n",
      "Epoch:  225 | train loss: 0.2575\n",
      "Epoch:  226 | train loss: 0.2574\n",
      "Epoch:  227 | train loss: 0.2574\n",
      "Epoch:  228 | train loss: 0.2573\n",
      "Epoch:  229 | train loss: 0.2573\n",
      "Epoch:  230 | train loss: 0.2572\n",
      "Epoch:  231 | train loss: 0.2572\n",
      "Epoch:  232 | train loss: 0.2572\n",
      "Epoch:  233 | train loss: 0.2571\n",
      "Epoch:  234 | train loss: 0.2571\n",
      "Epoch:  235 | train loss: 0.2570\n",
      "Epoch:  236 | train loss: 0.2570\n",
      "Epoch:  237 | train loss: 0.2569\n",
      "Epoch:  238 | train loss: 0.2569\n",
      "Epoch:  239 | train loss: 0.2568\n",
      "Epoch:  240 | train loss: 0.2568\n",
      "Epoch:  241 | train loss: 0.2567\n",
      "Epoch:  242 | train loss: 0.2567\n",
      "Epoch:  243 | train loss: 0.2566\n",
      "Epoch:  244 | train loss: 0.2566\n",
      "Epoch:  245 | train loss: 0.2566\n",
      "Epoch:  246 | train loss: 0.2565\n",
      "Epoch:  247 | train loss: 0.2565\n",
      "Epoch:  248 | train loss: 0.2564\n",
      "Epoch:  249 | train loss: 0.2564\n",
      "Epoch:  250 | train loss: 0.2563\n",
      "Epoch:  251 | train loss: 0.2563\n",
      "Epoch:  252 | train loss: 0.2562\n",
      "Epoch:  253 | train loss: 0.2562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  254 | train loss: 0.2561\n",
      "Epoch:  255 | train loss: 0.2561\n",
      "Epoch:  256 | train loss: 0.2560\n",
      "Epoch:  257 | train loss: 0.2560\n",
      "Epoch:  258 | train loss: 0.2559\n",
      "Epoch:  259 | train loss: 0.2559\n",
      "Epoch:  260 | train loss: 0.2558\n",
      "Epoch:  261 | train loss: 0.2558\n",
      "Epoch:  262 | train loss: 0.2557\n",
      "Epoch:  263 | train loss: 0.2557\n",
      "Epoch:  264 | train loss: 0.2556\n",
      "Epoch:  265 | train loss: 0.2556\n",
      "Epoch:  266 | train loss: 0.2556\n",
      "Epoch:  267 | train loss: 0.2555\n",
      "Epoch:  268 | train loss: 0.2555\n",
      "Epoch:  269 | train loss: 0.2554\n",
      "Epoch:  270 | train loss: 0.2554\n",
      "Epoch:  271 | train loss: 0.2553\n",
      "Epoch:  272 | train loss: 0.2553\n",
      "Epoch:  273 | train loss: 0.2552\n",
      "Epoch:  274 | train loss: 0.2552\n",
      "Epoch:  275 | train loss: 0.2551\n",
      "Epoch:  276 | train loss: 0.2551\n",
      "Epoch:  277 | train loss: 0.2550\n",
      "Epoch:  278 | train loss: 0.2550\n",
      "Epoch:  279 | train loss: 0.2549\n",
      "Epoch:  280 | train loss: 0.2549\n",
      "Epoch:  281 | train loss: 0.2548\n",
      "Epoch:  282 | train loss: 0.2548\n",
      "Epoch:  283 | train loss: 0.2547\n",
      "Epoch:  284 | train loss: 0.2547\n",
      "Epoch:  285 | train loss: 0.2546\n",
      "Epoch:  286 | train loss: 0.2546\n",
      "Epoch:  287 | train loss: 0.2545\n",
      "Epoch:  288 | train loss: 0.2545\n",
      "Epoch:  289 | train loss: 0.2544\n",
      "Epoch:  290 | train loss: 0.2544\n",
      "Epoch:  291 | train loss: 0.2543\n",
      "Epoch:  292 | train loss: 0.2543\n",
      "Epoch:  293 | train loss: 0.2542\n",
      "Epoch:  294 | train loss: 0.2542\n",
      "Epoch:  295 | train loss: 0.2541\n",
      "Epoch:  296 | train loss: 0.2541\n",
      "Epoch:  297 | train loss: 0.2540\n",
      "Epoch:  298 | train loss: 0.2540\n",
      "Epoch:  299 | train loss: 0.2539\n",
      "tensor([[-0.9940]], grad_fn=<SelectBackward0>)  :  tensor([-0.9931])\n",
      "tensor([[-0.9931]], grad_fn=<SelectBackward0>)  :  tensor([-0.9966])\n",
      "tensor([[-0.9938]], grad_fn=<SelectBackward0>)  :  tensor([-0.9896])\n",
      "tensor([[-0.9943]], grad_fn=<SelectBackward0>)  :  tensor([-0.9966])\n",
      "tensor([[-0.9938]], grad_fn=<SelectBackward0>)  :  tensor([-0.9896])\n",
      "tensor([[-0.9938]], grad_fn=<SelectBackward0>)  :  tensor([-0.9896])\n",
      "tensor([[-0.9938]], grad_fn=<SelectBackward0>)  :  tensor([-0.9896])\n",
      "tensor([[-0.9938]], grad_fn=<SelectBackward0>)  :  tensor([-0.9896])\n",
      "tensor([[-0.9956]], grad_fn=<SelectBackward0>)  :  tensor([-0.9966])\n",
      "tensor([[-0.9965]], grad_fn=<SelectBackward0>)  :  tensor([-0.9931])\n"
     ]
    }
   ],
   "source": [
    "#had big issue with tensor shape for outputs - undetected for a while\n",
    "\n",
    "# training and testing\n",
    "size = np.size(outputs[0])\n",
    "print(np.shape(outputs[0].view(listSplitValue, 1, 1)))\n",
    "for epoch in range(300):\n",
    "    for x in range(1):        # gives batch data\n",
    "        output = rnn(inputs[x])                               # rnn output\n",
    "        loss = loss_fn(output, outputs[x].view(listSplitValue, 1, 1))                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "    print('Epoch: ', epoch, '| train loss: %.4f' % loss.data)\n",
    "    \n",
    "for x in range(10):\n",
    "    print(output[x], \" : \", outputs[0][x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
